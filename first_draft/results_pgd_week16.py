# -*- coding: utf-8 -*-
"""results_pgd_week16.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/198LCyZ-bqi9UK6NJMSuNL6DnSJULsTJn
"""

# !pip install foolbox
# from google.colab import drive
# import os
# drive.mount('/content/drive')
# # !pip install line_profiler

## All imports

import sys
import os
import random
import numpy as np
from PIL import Image
import json
import torch
import torchvision
import torchvision.transforms as transforms
import torch.nn.functional as F
import torch.nn as nn
import torch.optim as optim
from torch.utils.data.sampler import SubsetRandomSampler
from torch.utils.data import Dataset, DataLoader, TensorDataset
import torchvision.models as models

import torchvision.transforms as transforms
import torchvision.datasets as datasets

from math import sqrt, log2
from skimage import color
from skimage.segmentation import slic

from time import time
from shutil import copyfile
import sys
import argparse

import matplotlib.pyplot as plt
import numpy as np
import pickle
from time import time
import foolbox as fb
import copy

## Directories and experiment


exp_eps = int(sys.argv[1])
seed_no_loc = int(sys.argv[2])


#seed_no_loc = 1
#exp_eps = 1
exp_seg = 6


# Set number of workers

workers = 8


directory_net = "/cluster/scratch/pmayilvah/Week16_pgd/"+str(exp_eps)+"/"+str(seed_no_loc)+"/net/"

directory_save = "/cluster/scratch/pmayilvah/Week16_pgd_results/"+str(exp_eps)+"/"+str(seed_no_loc)+"/"
os.mkdir(directory_save)

## This is the list of segments that goes in slic (actual vs real)

# segments_stack_actual = [1, 4, 9, 16, 25, 36, 64, 120, 256, 1024]
# segments_stack = [1, 6, 10, 12, 30, 36, 70, 100, 300, 1024]

## For CIFAR10 we can simply pass the actual segmentents

segments_stack_actual = [32, 64, 128, 256, 448, 1024]
segments_stack = [32, 64, 128, 256, 448, 1024]

eps_stack_temp = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]

"""### Params = [GPU, Image Size, Dataset, Batch Size, Dataset Size, scale_capacity, epochs, epsilon, epsilon_stack, number_segments, learning_rate, attack, pgd_steps, is_resnet, PGD_training, segmentation_training, PGD_and_segmenter_training]

"""

## This is where all arguments are given


params = ["True", 32, "CIFAR", 125, 50000, 1, 200, eps_stack_temp[exp_eps-1], eps_stack_temp, segments_stack[exp_seg-1], 0.1, 'L2PGD', 20, True, True, False, False]

## device type and number of channels
def numChans(dataset):
    if dataset == "SC":
        return 1
    else:
        return 3

def numClasses(dataset):
    if dataset == "SC":
        return 2
    else:
        return 10

if params[0].lower() == 'true' and torch.cuda.is_available():
    device = torch.device("cuda:0")
    workers = 8
    print("Running on the GPU")
else:
    device = torch.device("cpu")
    workers = 0
    print("Running on the CPU")

## Setting seed

torch.manual_seed(3)

## Taking in all the given parameters and creating a dictionary and saving it

img_size = int(params[1])
dataset_type = params[2]
batch_size = int(params[3])
dataset_size = int(params[4])
scale_capacity = int(params[5])
nEpochs = params[6]
eps = params[7]
eps_stack = params[8]
nsegs = params[9] ## Definitely change this when using segmenter
nsegs_actual = params[9] ## Definitely change this when using segmenter

# nsegs = 1024
# nsegs_actual = 1024

learning_rate = params[10]
attack = params[11]
pgd_steps = params[12]
is_resnet = params[13]
PGD_training = params[14]
segmentation_training = params[15]
PGD_and_segmenter_training = params[16]
## number of channels
chans = numChans(dataset_type)
nClasses = numClasses(dataset_type)

sample_size = 5000 ## Need to give at least one batch otherwise code breaks.

## Saving all params in a dictionary

args = {
    'dataset_type': dataset_type,
    'dataset_size': dataset_size,
    'image_size': img_size,
    'PGD_training': PGD_training,
    'attack': attack,
    'epsilon': eps,
    'epsilon_stack': eps_stack,
    'pgd_steps': pgd_steps,
    'scale_capacity': scale_capacity,
    'Epochs': nEpochs,
    'batch_size': batch_size,
    'segmentation_training': segmentation_training,
    'number_of_segments': nsegs,
    'number_of_segments_actual': nsegs_actual,
    'learning_rate': learning_rate,
    'nClasses': nClasses, 
    'device': device, 
    'number_of_channels': chans,
    'is_resnet': is_resnet,
    'PGD_and_segmenter_training': PGD_and_segmenter_training,
    'sample_size': sample_size
}

args_save = {
    'dataset_type': dataset_type,
    'dataset_size': dataset_size,
    'image_size': img_size,
    'PGD_training': PGD_training,
    'attack': attack,
    'epsilon': eps,
    'epsilon_stack': eps_stack,
    'pgd_steps': pgd_steps,
    'scale_capacity': scale_capacity,
    'Epochs': nEpochs,
    'batch_size': batch_size,
    'segmentation_training': segmentation_training,
    'number_of_segments': nsegs,
    'number_of_segments_actual': nsegs_actual,
    'learning_rate': learning_rate,
    'nClasses': nClasses,
    'is_resnet': is_resnet,
    'PGD_and_segmenter_training': PGD_and_segmenter_training,
    'sample_size': sample_size
}

##Creating all necessary folders and dumping args to params.txt

with open(directory_save+'params.txt', 'w') as file:
    json.dump(args_save, file)

args

"""

##**Square Circle dataset from data.py**

"""

## Function that smooths images based on segments (rewrite efficiently later)

def avg_seg(image,segs, size = args['image_size']):
    image = np.transpose(np.copy(image.numpy()),(1,2,0))
    sums = {}
    ns = {}
    for x in range(size):
        for y in range(size):
            zone = segs[x][y]
            if not zone in sums:
                sums[zone] = np.zeros_like(image[0][0])
                ns[zone] = 0
            sums[zone] += image[x][y]
            ns[zone] += 1
    for x in range(size):
        for y in range(size):
            zone = segs[x][y]
            image[x][y] = sums[zone]/ns[zone]
    return torch.tensor(np.transpose(image,(2,0,1)))

## Function that I created. Does what batch_seg does but for single image.It is used by square circle dataset
def singleSeg(img, nsegs, seg):
    return avg_seg(img.cpu(),seg(img.cpu(),nsegs))


## Segments a batch of images and returns a smoothed version
def batchSeg(images,nsegs,seg):
    tab = [avg_seg(img.cpu(),seg(img.cpu(),nsegs)) for img in images]
    return torch.stack(tab)

## SLIC function to segment image
def seg_SLIC(image,nsegs, compactness = 10.0, max_iter = 10):
    im = image.detach().numpy().astype('double').transpose(1,2,0)
    return slic(im,n_segments=nsegs, compactness = compactness, max_iter = max_iter)

#Loads dataset from files generated by gen_dataset()
def FROM_FILE(path, bs=128, augment_training=True, seed_transform = False):
    size = 32
    normalize = transforms.Normalize((0.5, 0.5, 0.5),(0.5, 0.5, 0.5))
    #normalize = transforms.Normalize((0., 0., 0),(1., 1., 1.))

    transform_train = transforms.Compose([
        transforms.ToPILImage(),
        transforms.RandomCrop(32, padding=4),
        transforms.RandomHorizontalFlip(),
        transforms.Resize(size, Image.NEAREST),
        transforms.ToTensor(),
        normalize
    ])

    transform_test = transforms.Compose([
        transforms.ToPILImage(),
        transforms.Resize(size, Image.NEAREST),
        transforms.ToTensor(),
        normalize
    ])

    transform_valid = transform_test

    if augment_training is False:
        transform_train = transform_test

    tr_dataset = FileDataset(path+"tr_untouched.pt", transform_train, seed_transform)
    va_dataset = FileDataset(path+"va_untouched.pt", transform_valid, seed_transform)
    te_dataset = FileDataset(path+"te_untouched.pt", transform_test, seed_transform)


    tr_loader = DataLoader(tr_dataset, batch_size=bs)
    va_loader = DataLoader(va_dataset, batch_size=bs)
    te_loader = DataLoader(te_dataset, batch_size=bs)

    return tr_loader, va_loader, te_loader



def FROM_FILE_both(path, bs=128, augment_training=True, seed_transform = False):
    size = 32
    normalize = transforms.Normalize((0.5, 0.5, 0.5),(0.5, 0.5, 0.5))
    #normalize = transforms.Normalize((0., 0., 0),(1., 1., 1.))

    transform_train = transforms.Compose([
        transforms.ToPILImage(),
        transforms.RandomCrop(32, padding=4),
        transforms.RandomHorizontalFlip(), # I don't do this because of difference between touched and untouched
        transforms.Resize(size, Image.NEAREST),
        transforms.ToTensor(),
        normalize
    ])

    transform_test = transforms.Compose([
        transforms.ToPILImage(),
        transforms.Resize(size, Image.NEAREST),
        transforms.ToTensor(),
        normalize
    ])

    transform_valid = transform_test

    if augment_training is False:
        transform_train = transform_test

    tr_dataset = FileDataset(path+"tr.pt", transform_train, seed_transform)
    va_dataset = FileDataset(path+"va.pt", transform_valid, seed_transform)
    te_dataset = FileDataset(path+"te.pt", transform_test, seed_transform)

    tr_dataset_untouched = FileDataset(path+"tr_untouched.pt", transform_train, seed_transform)
    va_dataset_untouched = FileDataset(path+"va_untouched.pt", transform_valid, seed_transform)
    te_dataset_untouched = FileDataset(path+"te_untouched.pt", transform_test, seed_transform)

    tr_loader = DataLoader(tr_dataset, batch_size=bs)
    va_loader = DataLoader(va_dataset, batch_size=bs)
    te_loader = DataLoader(te_dataset, batch_size=bs)

    tr_loader_untouched = DataLoader(tr_dataset_untouched, batch_size=bs)
    va_loader_untouched = DataLoader(va_dataset_untouched, batch_size=bs)
    te_loader_untouched = DataLoader(te_dataset_untouched, batch_size=bs)

    return tr_loader, va_loader, te_loader, tr_loader_untouched, va_loader_untouched, te_loader_untouched


class FileDataset(Dataset):
    def __init__(self,path,transform, seed_transform):
        self.images,self.labels=torch.load(path)
        self.transform=transform
        self.seed_transform = seed_transform

    def __getitem__(self,idx):
        if self.seed_transform:
            random.seed(3)
            torch.manual_seed(3)
        return self.transform((self.images[idx]+1.0)/2.0),self.labels[idx]

    def __len__(self):
        return len(self.images)

path = "/cluster/home/pmayilvah/datasets/"
#path = "/content/drive/My Drive/master_thesis/datasets/"

## Seeding the 
torch.manual_seed(3)

## Change augment training to False when using PGD training

## Seed transform isn't required if you are not going to save fc weights

if (not args['PGD_training']) and args['segmentation_training']:
  nsegs = args['number_of_segments']
  tr_loader, va_loader, te_loader, tr_loader_untouched, va_loader_untouched, te_loader_untouched =  FROM_FILE_both(path+str(nsegs)+"_", bs=args['batch_size'], augment_training=True)
  print("loaded segmented CIFAR10 dataset for Segmenter training")
elif (not args['segmentation_training']) and (args['PGD_training'] or args['PGD_and_segmenter_training']):
  nsegs = 1024
  tr_loader, va_loader, te_loader =  FROM_FILE(path+str(nsegs)+"_", bs=args['batch_size'], augment_training=True, seed_transform=False)
  print("loaded untouched CIFAR10 dataset for PGD training")
else:
  print("Something is wrong")

## Data loader dicts

if (not args['PGD_training']) and args['segmentation_training']:
    data_loader = {'train': tr_loader_untouched,
                  'test': te_loader_untouched,
                  'val': va_loader_untouched}

else: 
    data_loader = {'train': tr_loader,
              'test': te_loader,
              'val': va_loader}

"""#**Classes from models.py**

##**Main.py, statistics, and training**
"""

## Class to load Segmenter+resnet

class SegResnet(nn.Module):
    def __init__(self, resnet_model, args = args):
        super(SegResnet, self).__init__()
        self.resnet = resnet_model
        self.fcAvg = torch.zeros(args['batch_size'], 3072, 3072).to(args['device'])

    
    def forward(self, x):
        x = x.view(-1, 1, 1024*args['number_of_channels'])
        x = torch.matmul(x, self.fcAvg)
        x = x.view(-1,1*args['number_of_channels'],32,32)
        x = self.resnet(x)
        return x
    
    def check(self, x):
        x = x.view(-1, 1, 1024*args['number_of_channels'])
        x = torch.matmul(x, self.fcAvg)
        x = x.view(-1,1*args['number_of_channels'],32,32)
        return x

## Converts segmenter to tensor for a batch of images that can be added on top of the standard net (function only works for SC dataset, needs work for standard datasets)


def imgs2tensor_faster(net, imgs,n, device = device, dim = 1024, nchans = 1):
  net.fcAvg*=0 ## Very important  
  obs_segs = {}
  for obs in range(imgs.shape[0]):
    obs_segs[obs] = {}
    segs = seg_SLIC(imgs[obs, :].cpu(),n)
    segs = segs.flatten()
    if n == 1024:
        net.fcAvg[obs, np.arange(dim*nchans), np.arange(dim*nchans)] = 1
    else:
        for i in np.unique(segs):
            obs_segs[obs][i] = {}
            locations = np.where(segs == i)[0]
            number_of_pixels = len(locations)
            temp_col = np.tile(locations, number_of_pixels)
            temp_row = np.repeat(locations, number_of_pixels)
            
            if nchans == 3:
                col_idx = np.hstack((temp_col, temp_col+dim, temp_col+dim*2))
                row_idx = np.hstack((temp_row, temp_row+dim, temp_row+dim*2))
            elif nchans == 1:
                col_idx = temp_col
                row_idx = temp_row
            else:
                print("something's wrong")
                break                
            obs_segs[obs][i]['col'] = col_idx
            obs_segs[obs][i]['row'] = row_idx
            obs_segs[obs][i]['sc'] = 1/number_of_pixels
  
  ## Set values of tensor
  if n!=1024:
      for obs in obs_segs.keys():
            for i in obs_segs[obs].keys():
                net.fcAvg[obs, obs_segs[obs][i]['col'], obs_segs[obs][i]['row']] = obs_segs[obs][i]['sc']

## Functions to assist in the training process

## Computes the gradient norm of the input

def l2_norm_grads(images, labels, net, criterion, optimizer):
  images.requires_grad = True # Setting true so that gradients can be backproped to input
  net.train()
  temp_loss = criterion(net(images), labels)
  temp_loss.backward()
  input_grad = images.grad.clone()
  optimizer.zero_grad() # Make grads zero
  images.requires_grad = False
  
  expected_norm_grads = 0
  for grad_loc in range(input_grad.shape[0]):
    expected_norm_grads+=torch.norm(input_grad[grad_loc]).item()
  net.eval()
  input_grad = input_grad.cpu().numpy()
  return input_grad, expected_norm_grads/input_grad.shape[0]

## The dictionary of all losses, accuracies, gradient norms for natural images and corresponding adversaries. Also vulnerabilities of natural images

# Natural images
## Adversaries

eps_stack_inf = [0.0, 0.002, 0.004, 0.006, 0.008, 0.009, 0.01, 0.02, 0.03, 0.04, 0.05]
eps_stack_pgd = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]
eps_stack_fgsm = [0.0, 0.002, 0.004, 0.006, 0.008, 0.009, 0.01, 0.02, 0.03, 0.04, 0.05]

## Dict keys

eps_stack_dict = {'L2': eps_stack_pgd,
                  'Linf': eps_stack_inf,
                  'FGSM': eps_stack_fgsm
}
best_keys = ['best']
attack_keys = ['L2', 'Linf', 'FGSM']
data_keys = ['test']


## Getting the image dicts ready
adversaries_images = {}
normal_images_dict = {}

accuracies_adversaries = {}
vulnerabilities = {}

adv_norm_grads = {}
logit_images = {}
correct_labels = {}
input_grad_norms_stack = {}
input_grad_stack = {}
logit_advs = {}
advs_success_failures = {}

for b_key in best_keys:
    adversaries_images[b_key] = {}
    normal_images_dict[b_key] = {}

    accuracies_adversaries[b_key] = {}
    vulnerabilities[b_key] = {}

    adv_norm_grads[b_key] = {}
    logit_images[b_key] = {}
    correct_labels[b_key] = {}
    input_grad_norms_stack[b_key] = {}
    input_grad_stack[b_key] = {}
    logit_advs[b_key] = {}
    advs_success_failures[b_key] = {}


    for a_key in attack_keys :
        adversaries_images[b_key][a_key] = {}
        normal_images_dict[b_key][a_key] = {}

        accuracies_adversaries[b_key][a_key] = {}
        vulnerabilities[b_key][a_key] = {}    

        adv_norm_grads[b_key][a_key] = {}
        logit_images[b_key][a_key] = {}
        correct_labels[b_key][a_key] = {}
        input_grad_norms_stack[b_key][a_key] = {}
        input_grad_stack[b_key][a_key] = {}
        logit_advs[b_key][a_key] = {}
        advs_success_failures[b_key][a_key] = {}    

        for key in ['train', 'test', 'val']:
            adversaries_images[b_key][a_key][key] = None
            normal_images_dict[b_key][a_key][key] = None

            accuracies_adversaries[b_key][a_key][key] = {}
            vulnerabilities[b_key][a_key][key] = {} 

            adv_norm_grads[b_key][a_key][key] = {}
            logit_images[b_key][a_key][key] = {}
            correct_labels[b_key][a_key][key] = {}
            input_grad_norms_stack[b_key][a_key][key] = {}
            input_grad_stack[b_key][a_key][key] = {}
            logit_advs[b_key][a_key][key] = {}
            advs_success_failures[b_key][a_key][key] = {} 



## Function to save all statistics to colab

def save_dictionaries(directory):
  with open(directory+'vulnerabilities.pickle', 'wb') as handle:
      pickle.dump(vulnerabilities, handle, protocol=pickle.HIGHEST_PROTOCOL)

  with open(directory+'accuracies_adversaries.pickle', 'wb') as handle:
      pickle.dump(accuracies_adversaries, handle, protocol=pickle.HIGHEST_PROTOCOL)

  with open(directory+'adversaries_images.pickle', 'wb') as handle:
      pickle.dump(adversaries_images, handle, protocol=pickle.HIGHEST_PROTOCOL)
   
  with open(directory+'normal_images.pickle', 'wb') as handle:
      pickle.dump(normal_images_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)

  with open(directory+'adv_norm_grads.pickle', 'wb') as handle:
      pickle.dump(adv_norm_grads, handle, protocol=pickle.HIGHEST_PROTOCOL)

  with open(directory+'logit_images.pickle', 'wb') as handle:
      pickle.dump(logit_images, handle, protocol=pickle.HIGHEST_PROTOCOL)

  with open(directory+'correct_labels.pickle', 'wb') as handle:
      pickle.dump(correct_labels, handle, protocol=pickle.HIGHEST_PROTOCOL)

  with open(directory+'input_grad_norms_stack.pickle', 'wb') as handle:
      pickle.dump(input_grad_norms_stack, handle, protocol=pickle.HIGHEST_PROTOCOL)
   
  with open(directory+'input_grad_stack.pickle', 'wb') as handle:
      pickle.dump(input_grad_stack, handle, protocol=pickle.HIGHEST_PROTOCOL)

  with open(directory+'logit_advs.pickle', 'wb') as handle:
      pickle.dump(logit_advs, handle, protocol=pickle.HIGHEST_PROTOCOL)      

  with open(directory+'advs_success_failures.pickle', 'wb') as handle:
    pickle.dump(advs_success_failures, handle, protocol=pickle.HIGHEST_PROTOCOL)

def compute_statistics_segmenter(loader, net, criterion, PGD, optimizer, sample_size, eps_stack, args = args):
  device = args['device']
  nClasses = args['nClasses']
  nsegs = args['number_of_segments']
  is_resnet = args['is_resnet']

  if is_resnet:
      fseg_net = SegResnet(net).to(device) 
  else: 
      ## Loading fseg_nets parameters from standard net
      fseg_net = fseg_NetCCFC(nClasses, args['scale_capacity']).to(device)
      fseg_net.load_state_dict(net.state_dict(), strict=False)

  ## segmenter optimizer
  optimizer_fseg_net = optim.SGD(fseg_net.parameters(), lr=args['learning_rate'], momentum=0.9) ## Optimizer
  optimizer_fseg_net.zero_grad() # Making grads zero just in case
  optimizer.zero_grad() # Making grads zero just in case

  ## Set eval mode and convert to form which foolbox can access
  fseg_net.eval()
  net.eval()


  ## Initialize variables that accumulate statistics for each batch

  avg_vulnerabilities = {}
  avg_adv_loss = {}
  avg_adv_acc = {}
  avg_adv_norm_grads = {}
  
  adv_images = {}
  adv_segmented_images = {}
  advs_success_failures = {}
  logits_advs = {}

  input_grad_norms_stack = []

  ## Initializing zero for adversarial stats
  for i in eps_stack:
    avg_vulnerabilities[i] = 0
    avg_adv_loss[i] = 0
    avg_adv_acc[i] = 0
    avg_adv_norm_grads[i] = 0

  total_size = 0
  start_time = time()
  marker_save = 0 ## Getting a sample of 500 images

  for i, data in enumerate(loader):
    images, labels = data
    images = images.to(device)
    labels = labels.to(device)

    ## Initializing the segment layer in front for that particular batch of images

    imgs2tensor_faster(fseg_net, images, nsegs, device = device, dim = 1024, nchans = 3)
    
    ## Creating fmodel for attacking
    fmodel_fseg_net = fb.PyTorchModel(fseg_net, bounds=(-1, 1))
    
    total_size+=images.shape[0]
    
    ## Using this to break, so as to compute statistics only for a small subset of the datapoints
    if sample_size < total_size:
        break
    
    ## Printing stuff to monitor training process
    if i%10 == 0:
      print("number of processed batches = {}, images = {}, time_taken for 10 batches = {}".format(i, total_size, time() - start_time))
      start_time = time()


    ## Input grad stuff
    input_grad, input_grad_norms = l2_norm_grads(images, labels, fseg_net, criterion, optimizer_fseg_net)
    input_grad_norms_stack.append(input_grad_norms)

    if i == marker_save:
        if i == 0:
            normal_images = images.cpu().numpy()
            normal_segmented_images = fseg_net.check(images).cpu().numpy()
            logits_images = torch.nn.functional.softmax(fseg_net(images), dim = 1).cpu().detach().numpy()
            correct_labels = labels.cpu().numpy()

            ## image gradients
            input_grad_stack = input_grad


        else:
            normal_images = np.vstack((normal_images, images.cpu().numpy()))
            normal_segmented_images = np.vstack((normal_segmented_images, fseg_net.check(images).cpu().numpy()))
            logits_images = np.vstack((logits_images, torch.nn.functional.softmax(fseg_net(images), dim = 1).cpu().detach().numpy()))
            correct_labels = np.hstack((correct_labels, labels.cpu().numpy()))

            ## image gradients
            input_grad_stack = np.vstack((input_grad_stack, input_grad))



    # Adversaries stats
    for k in eps_stack:
        _, [advs], advs_success = PGD(fmodel_fseg_net, images, labels, epsilons=[k])
        optimizer_fseg_net.zero_grad()
        if i == marker_save:
            if i == 0:        
                ## Images: normal, segmented etc.
                adv_images[k] = advs.cpu().numpy() # Taking the first batch of adversaries for that particular epsilon essentially
                adv_segmented_images[k] = fseg_net.check(advs).cpu().numpy()

                ## Labels and logits
                advs_success_failures[k] = advs_success.cpu().numpy()[0, :]
                logits_advs[k] = torch.nn.functional.softmax(fseg_net(advs), dim = 1).cpu().detach().numpy()
            else:
                ## Images: normal, segmented etc.
                adv_images[k] = np.vstack((adv_images[k], advs.cpu().numpy())) 
                adv_segmented_images[k] = np.vstack((adv_segmented_images[k], fseg_net.check(advs).cpu().numpy()))

                ## Labels and logits
                advs_success_failures[k] = np.hstack((advs_success_failures[k], advs_success.cpu().numpy()[0, :]))
                logits_advs[k] = np.vstack((logits_advs[k], torch.nn.functional.softmax(fseg_net(advs), dim = 1).cpu().detach().numpy()))


        avg_vulnerabilities[k]+=advs_success.sum().item()
        # Zero grad
        optimizer_fseg_net.zero_grad()

        advs.requires_grad = True
        out_advs = fseg_net(advs)
        loss_advs = criterion(out_advs, labels)
        reps_advs = out_advs.argmax(1)
        acc_advs = ((labels == reps_advs).sum()).float()
        loss_advs.backward()
        
        avg_adv_loss[k]+=loss_advs.item()*len(advs)
        avg_adv_acc[k]+=acc_advs.item()

        advs_grad = advs.grad.clone()

        # Zero grad
        optimizer_fseg_net.zero_grad()

        advs.required_grad = False
        for j in range(advs_grad.shape[0]):
            avg_adv_norm_grads[k]+=torch.norm(advs_grad[j]).item()

  
  # Average out adversary stats
  for k in eps_stack:
      avg_adv_loss[k] = avg_adv_loss[k]/total_size
      avg_vulnerabilities[k] = avg_vulnerabilities[k]/total_size
      avg_adv_acc[k] = avg_adv_acc[k]/total_size
      avg_adv_norm_grads[k] = avg_adv_norm_grads[k]/total_size


  return avg_vulnerabilities, avg_adv_loss, avg_adv_acc, avg_adv_norm_grads, adv_images, adv_segmented_images, normal_images, normal_segmented_images, logits_images, correct_labels, input_grad_norms_stack, input_grad_stack, logits_advs, advs_success_failures

## This function is essentially a photocopy of the training chunk, except that we don't propagate gradients. Used to compute statistics of validation dataset every epoch

## Can pass all epsilons in one go, make this change later, instead of passing all in one go

def compute_statistics(loader, net, criterion, PGD, fmodel, optimizer, sample_size, eps_stack, args = args):
  optimizer.zero_grad() # Making grads zero just in case (paranoid)
  device = args['device']
  
  # Initialize variables that accumulate statistics for each batch

  ## Initialize variables that accumulate statistics for each batch

  avg_vulnerabilities = {}
  avg_adv_loss = {}
  avg_adv_acc = {}
  avg_adv_norm_grads = {}
  
  adv_images = {}
  advs_success_failures = {}
  logits_advs = {}

  input_grad_norms_stack = []  

  for i in eps_stack:
    avg_vulnerabilities[i] = 0
    avg_adv_loss[i] = 0
    avg_adv_acc[i] = 0
    avg_adv_norm_grads[i] = 0

  total_size = 0

  start_time = time()

  marker_save = 0 ## Getting a sample of 500 images

  for i, data in enumerate(loader):
    if i%10 == 0:
      print("number of processed batches = {}, images = {}, time_taken for 10 batches = {}".format(i, total_size, time() - start_time))
      start_time = time()

    images, labels = data
    images = images.to(device)
    labels = labels.to(device)

    total_size+=images.shape[0] ## Increasing total size
    
    ## Input grad stuff
    input_grad, input_grad_norms = l2_norm_grads(images, labels, net, criterion, optimizer)
    input_grad_norms_stack.append(input_grad_norms)   

    if i == marker_save:
        if i == 0:
            normal_images = images.cpu().numpy()
            logits_images = torch.nn.functional.softmax(net(images), dim = 1).cpu().detach().numpy()
            correct_labels = labels.cpu().numpy()

            ## image gradients
            input_grad_stack = input_grad


        else:
            normal_images = np.vstack((normal_images, images.cpu().numpy()))
            logits_images = np.vstack((logits_images, torch.nn.functional.softmax(net(images), dim = 1).cpu().detach().numpy()))
            correct_labels = np.hstack((correct_labels, labels.cpu().numpy()))

            ## image gradients
            input_grad_stack = np.vstack((input_grad_stack, input_grad))    

    # Adversaries stats (can pass all epsilons in one go, make this change later)
    for k in eps_stack:
      _, [advs], advs_success = PGD(fmodel, images, labels, epsilons=[k])
      
      optimizer.zero_grad()

      if i == marker_save:
          if i == 0:        
              ## Images: normal, segmented etc.
              adv_images[k] = advs.cpu().numpy() # Taking the first batch of adversaries for that particular epsilon essentially

              ## Labels and logits
              advs_success_failures[k] = advs_success.cpu().numpy()[0, :]
              logits_advs[k] = torch.nn.functional.softmax(net(advs), dim = 1).cpu().detach().numpy()
          else:
              ## Images: normal, segmented etc.
              adv_images[k] = np.vstack((adv_images[k], advs.cpu().numpy())) 

              ## Labels and logits
              advs_success_failures[k] = np.hstack((advs_success_failures[k], advs_success.cpu().numpy()[0, :]))
              logits_advs[k] = np.vstack((logits_advs[k], torch.nn.functional.softmax(net(advs), dim = 1).cpu().detach().numpy()))

      avg_vulnerabilities[k]+=advs_success.sum().item()
      optimizer.zero_grad()

      # Adversarial images stats and advesarial input gradients

      advs.requires_grad = True
      out_advs = net(advs)
      loss_advs = criterion(out_advs, labels)
      reps_advs = out_advs.argmax(1)
      acc_advs = ((labels == reps_advs).sum()).float()
      loss_advs.backward()
      
      avg_adv_loss[k]+=loss_advs.item()*len(advs) # Because loss is mean okay!
      avg_adv_acc[k]+=acc_advs.item()

      advs_grad = advs.grad.clone()
      optimizer.zero_grad()
      advs.required_grad = False
      for j in range(advs_grad.shape[0]):
        avg_adv_norm_grads[k]+=torch.norm(advs_grad[j]).item()
    
    # Using this to break, so as to compute statistics only for a small subset of the datapoints
    if sample_size < total_size:
        break

  # Average out adversary stats
  for k in eps_stack:
    avg_adv_loss[k] = avg_adv_loss[k]/total_size
    avg_vulnerabilities[k] = avg_vulnerabilities[k]/total_size
    avg_adv_acc[k] = avg_adv_acc[k]/total_size
    avg_adv_norm_grads[k] = avg_adv_norm_grads[k]/total_size

  return avg_vulnerabilities, avg_adv_loss, avg_adv_acc, avg_adv_norm_grads, adv_images, normal_images, logits_images, correct_labels, input_grad_norms_stack, input_grad_stack, logits_advs, advs_success_failures

"""### Main

"""

## PGD

PGD_l2 = fb.attacks.L2PGD(steps=args['pgd_steps'])
PGD_linf = fb.attacks.LinfPGD(steps=args['pgd_steps'])
FGSM = fb.attacks.FGSM()

PGD_dict = {
    'L2': PGD_l2,
    'Linf': PGD_linf,
    'FGSM': FGSM
}
sample_size = 50000

## Loading stuff necessary for computing training statistics 

net = models.resnet18().to(device)
net.fc = nn.Linear(512, args['nClasses']).to(device)

net.eval()
criterion = nn.CrossEntropyLoss() ## the loss function
optimizer = optim.SGD(net.parameters(), lr=args['learning_rate'], momentum=0.9, weight_decay=5e-4) ## Optimizer

# ## Compute all_stats
# for b_key in best_keys:
#     net_weights_loc = directory_net+b_key+".pt"
#     if os.path.isfile(net_weights_loc):
#         net.load_state_dict(torch.load(net_weights_loc))
        
#         for a_key in attack_keys:
#             PGD = PGD_dict[a_key]
#             eps_stack = eps_stack_dict[a_key]

#             for key in data_keys:
#                 d_loader = data_loader[key]
#                 vulnerabilities[b_key][a_key][key], _, accuracies_adversaries[b_key][a_key][key], adv_norm_grads[b_key][a_key][key], adversaries_images[b_key][a_key][key], adversaries_segmented_images[b_key][a_key][key], normal_images_dict[b_key][a_key][key], normal_segmented_images[b_key][a_key][key], logit_images[b_key][a_key][key], correct_labels[b_key][a_key][key], input_grad_norms_stack[b_key][a_key][key], input_grad_stack[b_key][a_key][key], logit_advs[b_key][a_key][key], advs_success_failures[b_key][a_key][key] = compute_statistics_segmenter(d_loader, net, criterion, PGD, optimizer, sample_size, eps_stack, args = args)
#                 save_dictionaries(directory_save)
#     else:
#         continue

## Compute all_stats
fmodel = fb.PyTorchModel(net, bounds=(-1, 1))
for b_key in best_keys:
    net_weights_loc = directory_net+b_key+".pt"
    if os.path.isfile(net_weights_loc):
        net.load_state_dict(torch.load(net_weights_loc))
        
        for a_key in attack_keys:
            PGD = PGD_dict[a_key]
            eps_stack = eps_stack_dict[a_key]

            for key in data_keys:
                d_loader = data_loader[key]
                vulnerabilities[b_key][a_key][key], _, accuracies_adversaries[b_key][a_key][key], adv_norm_grads[b_key][a_key][key], adversaries_images[b_key][a_key][key], normal_images_dict[b_key][a_key][key], logit_images[b_key][a_key][key], correct_labels[b_key][a_key][key], input_grad_norms_stack[b_key][a_key][key], input_grad_stack[b_key][a_key][key], logit_advs[b_key][a_key][key], advs_success_failures[b_key][a_key][key] = compute_statistics(d_loader, net, criterion, PGD, fmodel, optimizer, sample_size, eps_stack, args = args)
                save_dictionaries(directory_save)
    else:
        continue

save_dictionaries(directory_save)