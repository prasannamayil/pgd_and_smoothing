# -*- coding: utf-8 -*-
"""segmenter_week16.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13vqMOZus9HbKCYwTrzijcY3xtOYmZyTm
"""

# !pip install foolbox
# from google.colab import drive
# import os
# drive.mount('/content/drive')
# #!pip install line_profiler

# %load_ext line_profiler

## All imports

import sys
import os
import random
import numpy as np
from PIL import Image
import json
import torch
import torchvision
import torchvision.transforms as transforms
import torch.nn.functional as F
import torch.nn as nn
import torch.optim as optim
from torch.utils.data.sampler import SubsetRandomSampler
from torch.utils.data import Dataset, DataLoader, TensorDataset
import torchvision.models as models

import torchvision.transforms as transforms
import torchvision.datasets as datasets

from math import sqrt, log2
from skimage import color
from skimage.segmentation import slic

from time import time
from shutil import copyfile
import sys
import argparse

import matplotlib.pyplot as plt
import numpy as np
import pickle
from time import time
import foolbox as fb
import copy

## Directories and experiment

exp_seg = int(sys.argv[1])
seed_no_loc = int(sys.argv[2])

# exp_seg = 5 ## Always 6 for single PGD
# seed_no_loc = 1

exp_eps = 1 ## Always 1 for single segmenter

# Set number of workers

workers = 8


directory = "/cluster/scratch/pmayilvah/Week16_segmenter/"+str(exp_seg)+"/"+str(seed_no_loc)+"/"
directory_net = directory+"net/"

os.mkdir(directory)
os.mkdir(directory_net)

## This is the list of segments that goes in slic (actual vs real)

# segments_stack_actual = [1, 4, 9, 16, 25, 36, 64, 120, 256, 1024]
# segments_stack = [1, 6, 10, 12, 30, 36, 70, 100, 300, 1024]

## For CIFAR10 we can simply pass the actual segmentents

segments_stack_actual = [32, 64, 128, 256, 448, 1024]
segments_stack = [32, 64, 128, 256, 448, 1024]

eps_stack_temp = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]

seed_no_stack = [2, 4, 8, 16, 32, 64, 128, 256, 512, 1024]
seed_no = seed_no_stack[seed_no_loc-1]

"""### Params = [GPU, Image Size, Dataset, Batch Size, Dataset Size, scale_capacity, epochs, epsilon, epsilon_stack, number_segments, learning_rate, attack, pgd_steps, is_resnet, PGD_training, segmentation_training, PGD_and_segmenter_training]

"""

## This is where all arguments are given


params = ["True", 32, "CIFAR", 125, 50000, 1, 250, eps_stack_temp[exp_eps-1], [eps_stack_temp[exp_eps-1]], segments_stack[exp_seg-1], 0.1, 'L2PGD', 20, True, False, True, False, seed_no]

## device type and number of channels
def numChans(dataset):
    if dataset == "SC":
        return 1
    else:
        return 3

def numClasses(dataset):
    if dataset == "SC":
        return 2
    else:
        return 10

if params[0].lower() == 'true' and torch.cuda.is_available():
    device = torch.device("cuda:0")
    workers = 8
    print("Running on the GPU")
else:
    device = torch.device("cpu")
    workers = 0
    print("Running on the CPU")

## Taking in all the given parameters and creating a dictionary and saving it

img_size = int(params[1])
dataset_type = params[2]
batch_size = int(params[3])
dataset_size = int(params[4])
scale_capacity = int(params[5])
nEpochs = params[6]
eps = params[7]
eps_stack = params[8]
nsegs = params[9] ## Definitely change this when using segmenter
nsegs_actual = params[9] ## Definitely change this when using segmenter

seed_no = params[-1]

# nsegs = 1024
# nsegs_actual = 1024

learning_rate = params[10]
attack = params[11]
pgd_steps = params[12]
is_resnet = params[13]
PGD_training = params[14]
segmentation_training = params[15]
PGD_and_segmenter_training = params[16]
## number of channels
chans = numChans(dataset_type)
nClasses = numClasses(dataset_type)

sample_size = 5000 ## Need to give at least one batch otherwise code breaks.

## Saving all params in a dictionary

params_dict = {
    'dataset_type': dataset_type,
    'dataset_size': dataset_size,
    'image_size': img_size,
    'PGD_training': PGD_training,
    'attack': attack,
    'epsilon': eps,
    'epsilon_stack': eps_stack,
    'pgd_steps': pgd_steps,
    'scale_capacity': scale_capacity,
    'Epochs': nEpochs,
    'batch_size': batch_size,
    'segmentation_training': segmentation_training,
    'number_of_segments': nsegs,
    'number_of_segments_actual': nsegs_actual,
    'learning_rate': learning_rate,
    'nClasses': nClasses, 
    'device': device, 
    'number_of_channels': chans,
    'is_resnet': is_resnet,
    'PGD_and_segmenter_training': PGD_and_segmenter_training,
    'sample_size': sample_size,
    'seed_no': seed_no
}

params_dict_save = {
    'dataset_type': dataset_type,
    'dataset_size': dataset_size,
    'image_size': img_size,
    'PGD_training': PGD_training,
    'attack': attack,
    'epsilon': eps,
    'epsilon_stack': eps_stack,
    'pgd_steps': pgd_steps,
    'scale_capacity': scale_capacity,
    'Epochs': nEpochs,
    'batch_size': batch_size,
    'segmentation_training': segmentation_training,
    'number_of_segments': nsegs,
    'number_of_segments_actual': nsegs_actual,
    'learning_rate': learning_rate,
    'nClasses': nClasses,
    'is_resnet': is_resnet,
    'PGD_and_segmenter_training': PGD_and_segmenter_training,
    'sample_size': sample_size,
    'seed_no': seed_no
}

##Creating all necessary folders and dumping params_dict to params.txt

with open(directory+'params.txt', 'w') as file:
    json.dump(params_dict_save, file)

params_dict

"""

##**Square Circle dataset from data.py**

"""

## Function that smooths images based on segments (rewrite efficiently later)

def avg_seg(image,segs, size = params_dict['image_size']):
    image = np.transpose(np.copy(image.numpy()),(1,2,0))
    sums = {}
    ns = {}
    for x in range(size):
        for y in range(size):
            zone = segs[x][y]
            if not zone in sums:
                sums[zone] = np.zeros_like(image[0][0])
                ns[zone] = 0
            sums[zone] += image[x][y]
            ns[zone] += 1
    for x in range(size):
        for y in range(size):
            zone = segs[x][y]
            image[x][y] = sums[zone]/ns[zone]
    return torch.tensor(np.transpose(image,(2,0,1)))

## Function that I created. Does what batch_seg does but for single image.It is used by square circle dataset
def singleSeg(img, nsegs, seg):
    return avg_seg(img.cpu(),seg(img.cpu(),nsegs))


## Segments a batch of images and returns a smoothed version
def batchSeg(images,nsegs,seg):
    tab = [avg_seg(img.cpu(),seg(img.cpu(),nsegs)) for img in images]
    return torch.stack(tab)

## SLIC function to segment image
def seg_SLIC(image,nsegs, compactness = 10.0, max_iter = 10):
    im = image.detach().numpy().astype('double').transpose(1,2,0)
    return slic(im,n_segments=nsegs, compactness = compactness, max_iter = max_iter)

# # # Code chunk for checking if the dataloaders, singleSeg, batchSeg are working properly

# k = 37

# plt.imshow(singleSeg(next(enumerate(tr_loader_untouched))[1][0][k], params_dict['number_of_segments'], seg_SLIC).cpu().numpy()[0, :])

# plt.imshow(batchSeg(next(enumerate(tr_loader_untouched))[1][0], params_dict['number_of_segments'], seg_SLIC).cpu().numpy()[k,0, :])

# plt.imshow(next(enumerate(tr_loader_untouched))[1][0].cpu().numpy()[k,0, :])

# plt.imshow(next(enumerate(tr_loader))[1][0].cpu().numpy()[k,0, :])

#Generates a dataset by applying a transform to every image in the input data

def gen_dataset(loader,nsegs, file):
    out=torch.Tensor()
    labs=torch.LongTensor()
    for i,data in enumerate(loader):
        print(i)
        images,labels=data
        out=torch.cat((out,batchSeg(images, nsegs, seg_SLIC)))#transform applied to the data
        labs=torch.cat((labs, labels))
    torch.save((out, labs), file)

def gen_dataset_both(loader,nsegs, path1, path2):
    out=torch.Tensor()
    out_untouched=torch.Tensor()
    labs=torch.LongTensor()
    for i,data in enumerate(loader):
        print(i)
        images,labels=data
        out=torch.cat((out,batchSeg(images, nsegs, seg_SLIC)))#transform applied to the data
        out_untouched = torch.cat((out_untouched,images))
        print(out_untouched.shape)
        labs=torch.cat((labs, labels))
    torch.save((out, labs), path1)
    torch.save((out_untouched, labs), path2)

#Loads dataset from files generated by gen_dataset()
def FROM_FILE(path, bs=128, augment_training=True, seed_transform = False, shuffle = True):
    size = 32
    normalize = transforms.Normalize((0.5, 0.5, 0.5),(0.5, 0.5, 0.5))
    #normalize = transforms.Normalize((0., 0., 0),(1., 1., 1.))

    transform_train = transforms.Compose([
        transforms.ToPILImage(),
        transforms.RandomCrop(32, padding=4),
        transforms.RandomHorizontalFlip(),
        transforms.Resize(size, Image.NEAREST),
        transforms.ToTensor(),
        normalize
    ])

    transform_test = transforms.Compose([
        transforms.ToPILImage(),
        transforms.Resize(size, Image.NEAREST),
        transforms.ToTensor(),
        normalize
    ])

    transform_valid = transform_test

    if augment_training is False:
        transform_train = transform_test

    tr_dataset = FileDataset(path+"tr_untouched.pt", transform_train, seed_transform)
    va_dataset = FileDataset(path+"va_untouched.pt", transform_valid, seed_transform)
    te_dataset = FileDataset(path+"te_untouched.pt", transform_test, seed_transform)


    tr_loader = DataLoader(tr_dataset, batch_size=bs, shuffle = shuffle)
    va_loader = DataLoader(va_dataset, batch_size=bs, shuffle = shuffle)
    te_loader = DataLoader(te_dataset, batch_size=bs, shuffle = shuffle)

    return tr_loader, va_loader, te_loader



def FROM_FILE_both(path, bs=128, augment_training=True, seed_transform = False, shuffle = True):
    size = 32
    normalize = transforms.Normalize((0.5, 0.5, 0.5),(0.5, 0.5, 0.5))
    #normalize = transforms.Normalize((0., 0., 0),(1., 1., 1.))

    transform_train = transforms.Compose([
        transforms.ToPILImage(),
        transforms.RandomCrop(32, padding=4),
        transforms.RandomHorizontalFlip(), # I don't do this because of difference between touched and untouched
        transforms.Resize(size, Image.NEAREST),
        transforms.ToTensor(),
        normalize
    ])

    transform_test = transforms.Compose([
        transforms.ToPILImage(),
        transforms.Resize(size, Image.NEAREST),
        transforms.ToTensor(),
        normalize
    ])

    transform_valid = transform_test

    if augment_training is False:
        transform_train = transform_test

    tr_dataset = FileDataset(path+"tr.pt", transform_train, seed_transform)
    va_dataset = FileDataset(path+"va.pt", transform_valid, seed_transform)
    te_dataset = FileDataset(path+"te.pt", transform_test, seed_transform)

    tr_dataset_untouched = FileDataset(path+"tr_untouched.pt", transform_train, seed_transform)
    va_dataset_untouched = FileDataset(path+"va_untouched.pt", transform_valid, seed_transform)
    te_dataset_untouched = FileDataset(path+"te_untouched.pt", transform_test, seed_transform)

    tr_loader = DataLoader(tr_dataset, batch_size=bs, shuffle = shuffle)
    va_loader = DataLoader(va_dataset, batch_size=bs, shuffle = shuffle)
    te_loader = DataLoader(te_dataset, batch_size=bs, shuffle = shuffle)

    tr_loader_untouched = DataLoader(tr_dataset_untouched, batch_size=bs, shuffle = shuffle)
    va_loader_untouched = DataLoader(va_dataset_untouched, batch_size=bs, shuffle = shuffle)
    te_loader_untouched = DataLoader(te_dataset_untouched, batch_size=bs, shuffle = shuffle)

    return tr_loader, va_loader, te_loader, tr_loader_untouched, va_loader_untouched, te_loader_untouched


class FileDataset(Dataset):
    def __init__(self,path,transform, seed_transform):
        self.images,self.labels=torch.load(path)
        self.transform=transform
        self.seed_transform = seed_transform

    def __getitem__(self,idx):
        if self.seed_transform:
            random.seed(3)
            torch.manual_seed(3)
        return self.transform((self.images[idx]+1.0)/2.0),self.labels[idx]

    def __len__(self):
        return len(self.images)

path = "/cluster/home/pmayilvah/datasets/"
#path = "/content/drive/My Drive/master_thesis/datasets/"

## Seeding the 
torch.manual_seed(params_dict['seed_no'])

## Change augment training to False when using PGD training

## Seed transform isn't required if you are not going to save fc weights

if (not params_dict['PGD_training']) and params_dict['segmentation_training']:
  nsegs = params_dict['number_of_segments']
  tr_loader, va_loader, te_loader, tr_loader_untouched, va_loader_untouched, te_loader_untouched =  FROM_FILE_both(path+str(nsegs)+"_", bs=params_dict['batch_size'], augment_training=True)
  print("loaded segmented CIFAR10 dataset for Segmenter training")
elif (not params_dict['segmentation_training']) and (params_dict['PGD_training'] or params_dict['PGD_and_segmenter_training']):
  nsegs = 1024
  tr_loader, va_loader, te_loader =  FROM_FILE(path+str(nsegs)+"_", bs=params_dict['batch_size'], augment_training=True, seed_transform=False)
  print("loaded untouched CIFAR10 dataset for PGD training")
else:
  print("Something is wrong")

## Code chunk to check slic's number of possible segments

# images, labels = next(enumerate(tr_loader_untouched))[1]
# nsegs = 16
# temp = 0
# for j, data in enumerate(va_loader_untouched):
#   images, labels = data
#   for i in range(125):
#       temp+=len(np.unique(seg_SLIC(images[i, :],nsegs, compactness = 10.0, max_iter = 10)))
#       #print(len(np.unique(seg_SLIC(images[i, :],nsegs, compactness = 10.0, max_iter = 10))))

# print(temp/5000)

# ## Code chunk to check if shuffle is working

# for i, data in enumerate(va_loader):
#   if i == 0:
#       images, labels = data
#   elif i%10==0:
#       print(i)

#plt.imshow(images[1, :].permute(1, 2, 0).numpy())

# k = 10
# path = "/content/drive/My Drive/master_thesis/datasets/"
# tr_dataset = torch.load(path+"64_"+"tr.pt")
# tr_loader_untouched = DataLoader(tr_dataset, batch_size=128)
# #temp_batch = next(enumerate(tr_loader))[1][0]
# plt.imshow(tr_dataset[0][k, :].permute(1, 2, 0).numpy())

"""#**Classes from models.py**

##**Main.py, statistics, and training**
"""

## We do apply dilation (do make note)

applyDilation = False ## Changing it to false 
if applyDilation and params_dict['image_size']>=32:
    n = int(log2(params_dict['image_size']/32))
    d1 = pow(2,(n+1)//2)
    d2 = pow(2,n//2)
else:
    d1 = 1
    d2 = 1

sfn = params_dict['image_size']//4-d1-2*d2
 
## Initialization of weights in both Conv2D and Linear is by default Kaiming He's method 

## Added scale_capacity to accordingly increase complexity


class NetCCFC(nn.Module):
    def __init__(self,nClasses, scale_capacity):
        super(NetCCFC,self).__init__()
        self.scale_capacity = scale_capacity
        self.conv1 = nn.Conv2d(params_dict['number_of_channels'],8*scale_capacity,5,dilation=d1)
        self.conv2 = nn.Conv2d(8*scale_capacity,64*scale_capacity,5,dilation=d2)
        self.mp = nn.MaxPool2d(2,2)
        self.fc1 = nn.Linear(64*sfn*sfn*scale_capacity,120*scale_capacity)
        self.fc2 = nn.Linear(120*scale_capacity,84*scale_capacity)
        self.fc3 = nn.Linear(84*scale_capacity,nClasses)

    def forward(self,x):
        x = F.relu(self.conv1(x))
        x = self.mp(x)
        x = F.relu(self.conv2(x))
        x = self.mp(x)
        x = x.view(-1, 16 * 4 * sfn * sfn * self.scale_capacity)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x


## Only works for Square Circle, need to modify for CIFAR/general datasets

class fseg_NetCCFC(nn.Module):
    def __init__(self,nClasses, scale_capacity):
        super(fseg_NetCCFC,self).__init__()
        self.scale_capacity = scale_capacity
        self.conv1 = nn.Conv2d(params_dict['number_of_channels'],8*scale_capacity,5,dilation=d1)
        self.conv2 = nn.Conv2d(8*scale_capacity,64*scale_capacity,5,dilation=d2)
        self.mp = nn.MaxPool2d(2,2)
        self.fc1 = nn.Linear(64*sfn*sfn*scale_capacity,120*scale_capacity)
        self.fc2 = nn.Linear(120*scale_capacity,84*scale_capacity)
        self.fc3 = nn.Linear(84*scale_capacity,nClasses)
        self.fcAvg = None

    def forward(self,x):
        x = x.view(-1, 1, 1024*params_dict['number_of_channels'])
        x = torch.matmul(x, self.fcAvg)
        x = x.view(-1,params_dict['number_of_channels'],32,32)
        x = F.relu(self.conv1(x))
        x = self.mp(x)
        x = F.relu(self.conv2(x))
        x = self.mp(x)
        x = x.view(-1, 16 * 4 * sfn * sfn * self.scale_capacity)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x
    
    def check(self, x):
        x = x.view(-1, 1, 1024*params_dict['number_of_channels'])
        print(x.shape)
        x = torch.matmul(x, self.fcAvg)
        print(x.shape)
        x = x.view(-1,1*params_dict['number_of_channels'],32,32)
        print(x.shape)
        return x

## Class to load Segmenter+resnet

class SegResnet(nn.Module):
    def __init__(self, resnet_model, params_dict = params_dict):
        super(SegResnet, self).__init__()
        self.resnet = resnet_model
        self.fcAvg = torch.zeros(params_dict['batch_size'], 3072, 3072).to(params_dict['device'])

    
    def forward(self, x):
        x = x.view(-1, 1, 1024*params_dict['number_of_channels'])
        x = torch.matmul(x, self.fcAvg)
        x = x.view(-1,1*params_dict['number_of_channels'],32,32)
        x = self.resnet(x)
        return x
    
    def check(self, x):
        x = x.view(-1, 1, 1024*params_dict['number_of_channels'])
        x = torch.matmul(x, self.fcAvg)
        x = x.view(-1,1*params_dict['number_of_channels'],32,32)
        return x

## Converts segmenter to tensor for a batch of images that can be added on top of the standard net (function only works for SC dataset, needs work for standard datasets)
def imgs2tensor(imgs,n, device = device, dim = 1024, nchans = 1):
  temp = np.zeros((imgs.shape[0], dim*nchans, dim*nchans))
  for obs in range(imgs.shape[0]):
    start_time = time()
    segs = seg_SLIC(imgs[obs, :].cpu(),n)

    segs = segs.flatten()
    fc_weight = np.zeros((len(segs)*nchans, len(segs)*nchans))

    if n == 1024:
      fc_weight = np.identity(dim*nchans)
    else:
      for i in np.unique(segs):
        locations = np.where(segs == i)[0]
        number_of_pixels = len(locations)
        temp_col = np.tile(locations, number_of_pixels)
        temp_row = np.repeat(locations, number_of_pixels)

        col_idx = list(temp_col)
        row_idx = list(temp_row)
        for c in range(nchans-1):
            col_idx.extend(list(temp_col+(1024*(c+1))))
            row_idx.extend(list(temp_row+(1024*(c+1))))

        fc_weight[col_idx, row_idx] = 1/number_of_pixels

    temp[obs, :] = fc_weight
  temp = torch.Tensor(temp).to(device)
  return temp   

## Converts segmenter for one image to fully connected layer (Not using this anymore)
def img2fc(img,n, device = device, dim = 1024, nchans = 1):
    segs = seg_SLIC(img.cpu(),n)

    segs = segs.flatten()
    fc_weight = np.zeros((len(segs)*nchans, len(segs)*nchans))

    if n == 1024:
      fc_weight = np.identity(dim*nchans)
    else:
      for i in np.unique(segs):
        number_of_pixels = len(np.where(segs == i)[0])
        sc = 1/number_of_pixels
        col_idx = np.tile(np.where(segs == i)[0], number_of_pixels)
        row_idx = np.repeat(np.where(segs == i)[0], number_of_pixels)
        for c in range(nchans):
          fc_weight[(col_idx+dim*c, row_idx+dim*c)] = sc
    
    out = nn.Linear(dim*nchans,dim*nchans).to(device)
    out.bias.data = torch.zeros_like(out.bias.data)
    out.weight.data = torch.FloatTensor(fc_weight).to(device)
    return out


def imgs2tensor_fast(net, imgs,n, device = device, dim = 1024, nchans = 1):
  net.fcAvg*=0
  for obs in range(imgs.shape[0]):
    segs = seg_SLIC(imgs[obs, :].cpu(),n)
    segs = segs.flatten()
    if n == 1024:
        net.fcAvg[obs, np.arange(dim*nchans), np.arange(dim*nchans)] = 1
    else:
        for i in np.unique(segs):
            locations = np.where(segs == i)[0]
            number_of_pixels = len(locations)
            temp_col = np.tile(locations, number_of_pixels)
            temp_row = np.repeat(locations, number_of_pixels)
            
            if nchans == 3:
                col_idx = np.hstack((temp_col, temp_col+dim, temp_col+dim*2))
                row_idx = np.hstack((temp_row, temp_row+dim, temp_row+dim*2))
            elif nchans == 1:
                col_idx = temp_col
                row_idx = temp_row
            else:
                print("something's wrong")
                break                
            net.fcAvg[obs, col_idx, row_idx] = 1/number_of_pixels

def imgs2tensor_faster(net, imgs,n, device = device, dim = 1024, nchans = 1):
  net.fcAvg*=0 ## Very important
  
  obs_segs = {}
  for obs in range(imgs.shape[0]):
    obs_segs[obs] = {}
    segs = seg_SLIC(imgs[obs, :].cpu(),n)
    segs = segs.flatten()
    if n == 1024:
        net.fcAvg[obs, np.arange(dim*nchans), np.arange(dim*nchans)] = 1
    else:
        for i in np.unique(segs):
            obs_segs[obs][i] = {}
            locations = np.where(segs == i)[0]
            number_of_pixels = len(locations)
            temp_col = np.tile(locations, number_of_pixels)
            temp_row = np.repeat(locations, number_of_pixels)
            
            if nchans == 3:
                col_idx = np.hstack((temp_col, temp_col+dim, temp_col+dim*2))
                row_idx = np.hstack((temp_row, temp_row+dim, temp_row+dim*2))
            elif nchans == 1:
                col_idx = temp_col
                row_idx = temp_row
            else:
                print("something's wrong")
                break                
            obs_segs[obs][i]['col'] = col_idx
            obs_segs[obs][i]['row'] = row_idx
            obs_segs[obs][i]['sc'] = 1/number_of_pixels
  
  ## Set values of tensor
  if n!=1024:
      for obs in obs_segs.keys():
            for i in obs_segs[obs].keys():
                net.fcAvg[obs, obs_segs[obs][i]['col'], obs_segs[obs][i]['row']] = obs_segs[obs][i]['sc']

## Code chunk to see if segmenter is working properly

# resnet = models.resnet18().to(device)
# resnet.fc = nn.Linear(512, params_dict['nClasses']).to(device)
# net = SegResnet(resnet).to(device) 

# # Segmented batch
# temp_images, temp_labels = next(enumerate(tr_loader_untouched))[1]
# temp_images = temp_images.to(device) 
# temp_labels = temp_labels.to(device)

# nsegs = params_dict['number_of_segments']
# start_time = time()
# imgs2tensor_faster(net, temp_images, nsegs, device = device, dim = 1024, nchans = 3)
# print(f"time take = {time() - start_time}")
# segmented_images = net.check(temp_images)
# k = 10
# plt.imshow(temp_images[k, :].permute(1, 2, 0).cpu().numpy())
# plt.imshow(segmented_images[k, :].permute(1, 2, 0).cpu().numpy())

# segmenter_net_temp = fseg_NetCCFC(params_dict['nClasses'], params_dict['scale_capacity']).to(device)

# # Adding the imgs2tensor layer on top

# segmenter_net_temp.fcAvg = imgs2tensor(temp_images,nsegs, device = device, nchans = 3)
# torch.norm(segmenter_net_temp.fcAvg.to(torch.device("cpu")) - seg_net.fcAvg.to(torch.device("cpu")))

# images, labels = next(enumerate(tr_loader))[1]
# images = images.to(device)
# labels = labels.to(device)

# resnet = models.resnet18(pretrained=False)
# net = SegResnet(resnet)

# nsegs = 64

# ## Adding segmenter in front

# start_time = time()
# tempo1 = imgs2tensor(images, nsegs, device = device, nchans = 3)
# print("time taken = {}".format(time()-start_time))

# ## Adding segmenter in front
# start_time = time()
# tempo2 = imgs2tensor2(images, nsegs, device = device, nchans = 3)
# print("time taken = {}".format(time()-start_time))

# ## Adding segmenter in front
# start_time = time()
# _ = imgs2fc(images, nsegs, device = device, nchans = 3)
# print("time taken = {}".format(time()-start_time))

# net.eval()
# fmodel = fb.PyTorchModel(net, bounds=(-1, 1))
# PGD = fb.attacks.L2PGD(steps=pgd_steps)

# tempo2

# ## Finding adversaries for that particular epsilon and training


# start_time = time()
# for eps in eps_stack:
#   _, [advs], temp_success = PGD(fmodel, images, labels, epsilons=[eps])

# print("time taken = {}".format(time()-start_time))


# ## Finding adversaries for that particular epsilon and training


# start_time = time()
# _, advs, temp_success = PGD(fmodel, images, labels, epsilons=eps_stack)

# print("time taken = {}".format(time()-start_time))

# net.fcAvg = imgs2tensor(temp_images_untouched,params_dict['number_of_segments'], device = device, nchans = 3)

# net(temp_images_untouched).sum().backward()

# ## Code chunk for checking if segmenter is working properly

# # Creating and loading both the networks

# net_temp = NetCCFC(params_dict['nClasses'], params_dict['scale_capacity']).to(device)
# segmenter_net_temp = fseg_NetCCFC(params_dict['nClasses'], params_dict['scale_capacity']).to(device)
# segmenter_net_temp.load_state_dict(net_temp.state_dict(), strict=False)

# # Segmented batch
# temp_images, temp_labels = next(enumerate(tr_loader))[1]
# temp_images = temp_images.to(device) 
# temp_labels = temp_labels.to(device)

# # Unsegmented version of the same batch
# temp_images_untouched, temp_labels_untouched = next(enumerate(tr_loader_untouched))[1]
# temp_images_untouched = temp_images_untouched.to(device) 
# temp_labels_untouched = temp_labels_untouched.to(device)

# # Adding the imgs2tensor layer on top

# segmenter_net_temp.fcAvg = imgs2tensor(temp_images_untouched,params_dict['number_of_segments'], device = device, nchans = 3)

# # Checking if imgs2tensor works properly

# segmented_images = segmenter_net_temp.check(temp_images_untouched) # getting the output of the segmenter tensor opteration


# ## Visual examination
# # choose image number from batch to show 
# k = 35
# plt.imshow(segmented_images.cpu().numpy()[k,0, :])
# plt.imshow(temp_images.cpu().numpy()[k,0, :])

# # Differences between segmented image from segmenter network
# print(torch.sum(segmented_images - temp_images)) ## We get zero value


# # Checking difference in output layer of both the networks
# print(torch.sum(segmenter_net_temp(temp_images_untouched) - net_temp(temp_images))) ## We get zero value

# segmentation_images = next(enumerate(te_loader))[1][0]

# k = 35
# plt.imshow(segmentation_images[k, :].permute(1, 2, 0).cpu().numpy())

## Functions to assist in the training process

## Loading a network from a given poth

def load_net(path):
    net = NetCCFC(nClasses).to(device)
    net.load_state_dict(torch.load(path, map_location=device))
    net.eval()
    return net

## Computes the gradient norm of the input

def l2_norm_grads(images, labels, net, criterion, optimizer):
  images.requires_grad = True # Setting true so that gradients can be backproped to input
  net.train()
  temp_loss = criterion(net(images), labels)
  temp_loss.backward()
  input_grad = images.grad.clone()
  optimizer.zero_grad() # Make grads zero
  images.requires_grad = False
  
  expected_norm_grads = 0
  for j in range(input_grad.shape[0]):
    expected_norm_grads+=torch.norm(input_grad[j]).item()
  return expected_norm_grads/input_grad.shape[0]

## This function is essentially a photocopy of the training chunk, except that we don't propagate gradients. Used to compute statistics of validation dataset every epoch

## Can pass all epsilons in one go, make this change later, instead of passing all in one go

def compute_statistics(loader, net, criterion, PGD, fmodel, optimizer, sample_size, params_dict = params_dict):
  optimizer.zero_grad() # Making grads zero just in case (paranoid)
  eps_stack = params_dict['epsilon_stack']
  device = params_dict['device']
  
  # Initialize variables that accumulate statistics for each batch

  avg_vulnerabilities = {}
  avg_adv_loss = {}
  avg_adv_acc = {}
  avg_adv_norm_grads = {}
  adv_images = {}
  for i in eps_stack:
    avg_vulnerabilities[i] = 0
    avg_adv_loss[i] = 0
    avg_adv_acc[i] = 0
    avg_adv_norm_grads[i] = 0

  total_size = 0

  marker_save = 0
  start_time = time()
  for i, data in enumerate(loader):
    if i%10 == 0:
      print("number of processed batches = {}, images = {}, time_taken for 10 batches = {}".format(i, total_size, time() - start_time))
      start_time = time()

    images, labels = data
    images = images.to(device)
    labels = labels.to(device)

    total_size+=images.shape[0] ## Increasing total size


    # Adversaries stats (can pass all epsilons in one go, make this change later)
    for k in eps_stack:
      _, [advs], advs_success = PGD(fmodel, images, labels, epsilons=[k])
      
      if i == marker_save:
        adv_images[k] = advs.cpu().numpy() # Storing a particular batch of adversaries for that particular epsilon essentially
        normal_images = images.cpu().numpy()

      avg_vulnerabilities[k]+=advs_success.sum().item()
      optimizer.zero_grad()

      # Adversarial images stats and advesarial input gradients

      advs.requires_grad = True
      out_advs = net(advs)
      loss_advs = criterion(out_advs, labels)
      reps_advs = out_advs.argmax(1)
      acc_advs = ((labels == reps_advs).sum()).float()
      loss_advs.backward()
      
      avg_adv_loss[k]+=loss_advs.item()*len(advs) # Because loss is mean okay!
      avg_adv_acc[k]+=acc_advs.item()

      advs_grad = advs.grad.clone()
      optimizer.zero_grad()
      advs.required_grad = False
      for j in range(advs_grad.shape[0]):
        avg_adv_norm_grads[k]+=torch.norm(advs_grad[j]).item()
    
    # Using this to break, so as to compute statistics only for a small subset of the datapoints
    if sample_size < total_size:
        break

  # Average out adversary stats
  for k in eps_stack:
    avg_adv_loss[k] = avg_adv_loss[k]/total_size
    avg_vulnerabilities[k] = avg_vulnerabilities[k]/total_size
    avg_adv_acc[k] = avg_adv_acc[k]/total_size
    avg_adv_norm_grads[k] = avg_adv_norm_grads[k]/total_size

  return avg_vulnerabilities, avg_adv_loss, avg_adv_acc, avg_adv_norm_grads, adv_images, normal_images

## This function is essentially a photocopy of the training chunk, except that we don't propagate gradients. Used to compute statistics of validation dataset every epoch

## Can pass all epsilons in one go, make this change later, instead of passing all in one go

def compute_statistics_pgd_and_segmenter(loader, net, criterion, PGD, optimizer, sample_size, params_dict = params_dict):
  optimizer.zero_grad() # Making grads zero just in case (paranoid)
  eps_stack = params_dict['epsilon_stack'] ## Change eps_stack to simply something with only eps
  device = params_dict['device']
  nsegs = params_dict['number_of_segments']
  
  # Initialize variables that accumulate statistics for each batch

  avg_vulnerabilities = {}
  avg_adv_loss = {}
  avg_adv_acc = {}
  avg_adv_norm_grads = {}
  adv_images = {}
  adv_segmented_images = {}

  for i in eps_stack:
    avg_vulnerabilities[i] = 0
    avg_adv_loss[i] = 0
    avg_adv_acc[i] = 0
    avg_adv_norm_grads[i] = 0

  total_size = 0

  marker_save = 0
  start_time = time()
  for i, data in enumerate(loader):
    if i%10 == 0:
      print("number of processed batches = {}, images = {}, time_taken for 10 batches = {}".format(i, total_size, time() - start_time))
      start_time = time()

    images, labels = data
    images = images.to(device)
    labels = labels.to(device)

    total_size+=images.shape[0] ## Increasing total size
    
    # Using this to break, so as to compute statistics only for a small subset of the datapoints
    if sample_size < total_size:
        break


    ##net.fcAvg = imgs2tensor(images, nsegs, device = device, nchans = 3) ### 
    net.eval()  
    imgs2tensor_faster(net, images, nsegs, device = device, dim = 1024, nchans = 3)
    fmodel = fb.PyTorchModel(net, bounds=(-1, 1))
    

    # Adversaries stats (can pass all epsilons in one go, make this change later)
    for k in eps_stack:
      _, [advs], advs_success = PGD(fmodel, images, labels, epsilons=[k])
      
      if i == marker_save:
          adv_images[k] = advs.cpu().numpy() # Storing a particular batch of adversaries for that particular epsilon essentially
          adv_segmented_images[k] = net.check(advs).cpu().numpy()
          normal_images = images.cpu().numpy()
          normal_segmented_images = net.check(images).cpu().numpy()


      avg_vulnerabilities[k]+=advs_success.sum().item()
      optimizer.zero_grad()

      # Adversarial images stats and advesarial input gradients

      advs.requires_grad = True
      out_advs = net(advs)
      loss_advs = criterion(out_advs, labels)
      reps_advs = out_advs.argmax(1)
      acc_advs = ((labels == reps_advs).sum()).float()
      loss_advs.backward()
      
      avg_adv_loss[k]+=loss_advs.item()*len(advs) # Because loss is mean okay!
      avg_adv_acc[k]+=acc_advs.item()

      advs_grad = advs.grad.clone()
      optimizer.zero_grad()
      advs.required_grad = False
      for j in range(advs_grad.shape[0]):
        avg_adv_norm_grads[k]+=torch.norm(advs_grad[j]).item()


  # Average out adversary stats
  for k in eps_stack:
    avg_adv_loss[k] = avg_adv_loss[k]/total_size
    avg_vulnerabilities[k] = avg_vulnerabilities[k]/total_size
    avg_adv_acc[k] = avg_adv_acc[k]/total_size
    avg_adv_norm_grads[k] = avg_adv_norm_grads[k]/total_size

  return avg_vulnerabilities, avg_adv_loss, avg_adv_acc, avg_adv_norm_grads, adv_images, adv_segmented_images, normal_images, normal_segmented_images

def compute_statistics_segmenter(loader, net, criterion, PGD, optimizer, sample_size, natural_stats = True, params_dict = params_dict):
  eps_stack = params_dict['epsilon_stack']
  device = params_dict['device']
  nClasses = params_dict['nClasses']
  nsegs = params_dict['number_of_segments']
  is_resnet = params_dict['is_resnet']

  if is_resnet:
      fseg_net = SegResnet(net).to(device) 
  else: 
      ## Loading fseg_nets parameters from standard net
      fseg_net = fseg_NetCCFC(nClasses, params_dict['scale_capacity']).to(device)
      #fseg_net = fseg_small_net(nClasses).to(device) ## small_net
      fseg_net.load_state_dict(net.state_dict(), strict=False)

  ## segmenter optimizer
  optimizer_fseg_net = optim.SGD(fseg_net.parameters(), lr=params_dict['learning_rate'], momentum=0.9) ## Optimizer
  optimizer_fseg_net.zero_grad() # Making grads zero just in case
  optimizer.zero_grad() # Making grads zero just in case

  ## Set eval mode and convert to form which foolbox can access
  fseg_net.eval()
  net.eval()


  ## Initialize variables that accumulate statistics for each batch

  avg_vulnerabilities = {}
  avg_adv_loss = {}
  avg_adv_acc = {}
  avg_adv_norm_grads = {}
  adv_images = {}
  adv_segmented_images = {}

  ## Initializing zero for adversarial stats
  for i in eps_stack:
    avg_vulnerabilities[i] = 0
    avg_adv_loss[i] = 0
    avg_adv_acc[i] = 0
    avg_adv_norm_grads[i] = 0

  total_size = 0
  start_time = time()
  marker_save = 0
  for i, data in enumerate(loader):
    images, labels = data
    images = images.to(device)
    labels = labels.to(device)

    ## Initializing the segment layer in front for that particular batch of images
    #fseg_net.fcAvg = imgs2tensor(images,nsegs, device = device, nchans = 3)
    
    imgs2tensor_faster(fseg_net, images, nsegs, device = device, dim = 1024, nchans = 3)
    
    ## Creating fmodel for attacking
    fmodel_fseg_net = fb.PyTorchModel(fseg_net, bounds=(-1, 1))
    
    total_size+=images.shape[0]
    
    ## Using this to break, so as to compute statistics only for a small subset of the datapoints
    if sample_size < total_size:
        break
    
    ## Printing stuff to monitor training process
    if i%10 == 0:
      print("number of processed batches = {}, images = {}, time_taken for 10 batches = {}".format(i, total_size, time() - start_time))
      start_time = time()

    # Adversaries stats
    for k in eps_stack:
      _, [advs], advs_success = PGD(fmodel_fseg_net, images, labels, epsilons=[k])
      
      if i == marker_save:
        adv_images[k] = advs.cpu().numpy() # Taking the frrst batch of adversaries for that particular epsilon essentially
        adv_segmented_images[k] = fseg_net.check(advs).cpu().numpy()
        normal_images = images.cpu().numpy()
        normal_segmented_images = fseg_net.check(images).cpu().numpy()


      avg_vulnerabilities[k]+=advs_success.sum().item()
      # Zero grad
      optimizer_fseg_net.zero_grad()

      advs.requires_grad = True
      out_advs = fseg_net(advs)
      loss_advs = criterion(out_advs, labels)
      reps_advs = out_advs.argmax(1)
      acc_advs = ((labels == reps_advs).sum()).float()
      loss_advs.backward()
      
      avg_adv_loss[k]+=loss_advs.item()*len(advs)
      avg_adv_acc[k]+=acc_advs.item()

      advs_grad = advs.grad.clone()

      # Zero grad
      optimizer_fseg_net.zero_grad()

      advs.required_grad = False
      for j in range(advs_grad.shape[0]):
        avg_adv_norm_grads[k]+=torch.norm(advs_grad[j]).item()

  
  # Average out adversary stats
  for k in eps_stack:
    avg_adv_loss[k] = avg_adv_loss[k]/total_size
    avg_vulnerabilities[k] = avg_vulnerabilities[k]/total_size
    avg_adv_acc[k] = avg_adv_acc[k]/total_size
    avg_adv_norm_grads[k] = avg_adv_norm_grads[k]/total_size


  return avg_vulnerabilities, avg_adv_loss, avg_adv_acc, avg_adv_norm_grads, adv_images, adv_segmented_images, normal_images, normal_segmented_images

## The dictionary of all losses, accuracies, gradient norms for natural images and corresponding adversaries. Also vulnerabilities of natural images

# Natural images

losses = {'tr_batch': [],
          'tr_epoch': [],
          'va_epoch': [],
          'te_epoch': []
}

accuracies = {'tr_batch': [],
              'tr_epoch': [],
              'va_epoch': [],
              'te_epoch': []
}

grad_norms = {'tr_batch': [],
              'tr_epoch': [],
              'va_epoch': [],
              'te_epoch': []
}

## Adversaries

losses_adversaries = {}
accuracies_adversaries = {}
grad_norms_adversaries = {}
vulnerabilities = {}

adversaries_images = {}
for key in ['train', 'test', 'val']:
  adversaries_images[key] = None

adversaries_segmented_images = {}
for key in ['train', 'test', 'val']:
  adversaries_segmented_images[key] = None


normal_images_dict = {}
for key in ['train', 'test', 'val']:
  normal_images_dict[key] = None

normal_segmented_images = {}
for key in ['train', 'test', 'val']:
  normal_segmented_images[key] = None

for i in eps_stack:
  losses_adversaries[i] = {'tr_batch': [],
            'tr_epoch': [],
            'va_epoch': [],
            'te_epoch': []
  }

  accuracies_adversaries[i] = {'tr_batch': [],
                'tr_epoch': [],
                'va_epoch': [],
                'te_epoch': []
  }

  grad_norms_adversaries[i] = {'tr_batch': [],
                'tr_epoch': [],
                'va_epoch': [],
                'te_epoch': []
  }

  ## Vulnerability

  vulnerabilities[i] = {'tr_batch': [],
                    'tr_epoch': [],
                    'va_epoch': [],
                    'te_epoch': []
  }



## Function to save all statistics to colab

def save_dictionaries():
  with open(directory+'losses.pickle', 'wb') as handle:
      pickle.dump(losses, handle, protocol=pickle.HIGHEST_PROTOCOL)

  with open(directory+'accuracies.pickle', 'wb') as handle:
      pickle.dump(accuracies, handle, protocol=pickle.HIGHEST_PROTOCOL)

  with open(directory+'vulnerabilities.pickle', 'wb') as handle:
      pickle.dump(vulnerabilities, handle, protocol=pickle.HIGHEST_PROTOCOL)

  with open(directory+'losses_adversaries.pickle', 'wb') as handle:
      pickle.dump(losses_adversaries, handle, protocol=pickle.HIGHEST_PROTOCOL)

  with open(directory+'accuracies_adversaries.pickle', 'wb') as handle:
      pickle.dump(accuracies_adversaries, handle, protocol=pickle.HIGHEST_PROTOCOL)

  with open(directory+'grad_norms.pickle', 'wb') as handle:
      pickle.dump(grad_norms, handle, protocol=pickle.HIGHEST_PROTOCOL)

  with open(directory+'grad_norms_adversaries.pickle', 'wb') as handle:
      pickle.dump(grad_norms_adversaries, handle, protocol=pickle.HIGHEST_PROTOCOL)

### This function can both be used as a PGD training function, or segmentation training function

def train_segmenter(epochs, params_dict = params_dict, path=""):
    
    global tr_loader, va_loader, te_loader, tr_loader_untouched, va_loader_untouched, te_loader_untouched
    # Assigning params_dict to variables that we use
    
    device = params_dict['device']
    pgd_steps = params_dict['pgd_steps']
    segmentation_training = params_dict['segmentation_training']
    PGD_training = params_dict['PGD_training']
    eps_stack = params_dict['epsilon_stack']
    attack = params_dict['attack']
    scale_capacity = params_dict['scale_capacity']
    dataset_size = params_dict['dataset_size']
    eps = params_dict['epsilon']
    learning_rate = params_dict['learning_rate']
    is_resnet = params_dict['is_resnet']
    sample_size = params_dict['sample_size']


    print(f"Starting training with {attack}")

    if is_resnet:
        net = models.resnet18().to(device)
        net.fc = nn.Linear(512, params_dict['nClasses']).to(device)
    else:
        net = NetCCFC(nClasses, params_dict['scale_capacity']).to(device)

    net.train()

    ## Initialize loss function and optimizer
    #criterion = nn.CrossEntropyLoss(reduction='sum') ## the loss function
    criterion = nn.CrossEntropyLoss() ## the loss function
    optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=5e-4) ## Optimizer
    
    #scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[75, 150])
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)



    ## Load corresponding attack based on given argument
    if attack is None:
        pass
    elif attack == 'FGSM':
        PGD = fb.attacks.FGSM()
    elif attack == 'L2PGD':
        PGD = fb.attacks.L2PGD(steps=pgd_steps) # Do not know if the relative step size is huge!
    else:
        raise ValueError(f'Attack {attack} not known')

    for e in range(epochs+1):
        epoch_start_time = time()
        ## Accumulator stats initialize to zero

        tr_natural_loss = 0
        tr_natural_acc = 0
        tr_natural_grads = 0

        start_time_10 = time()

        for i, data in enumerate(tr_loader):
            images, labels = data
            images = images.to(device)
            labels = labels.to(device)
            dtrain = time()

            net.train()

            ## natural images losses and outputs
            out_natural = net(images)
            loss_natural = criterion(out_natural, labels)
            reps_natural = out_natural.argmax(1)
            acc_natural = ((labels == reps_natural).sum()).float()

            ## Losses, Accuracies, and Grad norms of natural images at each batch
            losses['tr_batch'].append(loss_natural.item())
            accuracies['tr_batch'].append(acc_natural.item()/len(images))
            tr_natural_loss+=loss_natural.item()*len(images) 
            tr_natural_acc+=acc_natural.item()
            
            grad_norms['tr_batch'].append(l2_norm_grads(images, labels, net, criterion, optimizer)) 
            tr_natural_grads+=grad_norms['tr_batch'][-1]*len(images) 

            ## If segmentation training then this call updates weights
            if segmentation_training == True and e != 0:
                loss_natural.backward()
                optimizer.step()
                optimizer.zero_grad()
     
            
            if (i+1)%50 == 0:
                print("i = {} Accuracy_natural = {} Loss_natural = {} Grad_norm_natural {}".format(i+1, accuracies['tr_batch'][-1], losses['tr_batch'][-1], grad_norms['tr_batch'][-1]))
                print(f"time taken for 50 batches: {time()-start_time_10}")
                start_time_10 = time()
        
        ## An epoch is over hence step scheduler
        if e!=0:
            scheduler.step()
        
        # This is rather approximate
        losses['tr_epoch'].append(tr_natural_loss/dataset_size)
        accuracies['tr_epoch'].append(tr_natural_acc/dataset_size)
        grad_norms['tr_epoch'].append(tr_natural_grads/dataset_size)


        ## Train set stats
        tr_vulnerabilities, tr_adv_loss, tr_adv_acc, tr_adv_norm_grads, tr_adv_images, tr_adv_segmented_images, tr_normal_images, tr_normal_segmented_images = compute_statistics_segmenter(tr_loader_untouched, net, criterion, PGD, optimizer, sample_size, False, params_dict = params_dict)
        for l in eps_stack:
            vulnerabilities[l]['tr_epoch'].append(tr_vulnerabilities[l])
            losses_adversaries[l]['tr_epoch'].append(tr_adv_loss[l])
            accuracies_adversaries[l]['tr_epoch'].append(tr_adv_acc[l])
            grad_norms_adversaries[l]['tr_epoch'].append(tr_adv_norm_grads[l])
        
        ## Validation set stats for this epoch
        va_vulnerabilities, va_adv_loss, va_adv_acc, va_adv_norm_grads, va_adv_images, va_adv_segmented_images, va_normal_images, va_normal_segmented_images = compute_statistics_segmenter(va_loader_untouched, net, criterion, PGD, optimizer, sample_size, True, params_dict = params_dict)

        for p in eps_stack:
            vulnerabilities[p]['va_epoch'].append(va_vulnerabilities[p])
            losses_adversaries[p]['va_epoch'].append(va_adv_loss[p])
            accuracies_adversaries[p]['va_epoch'].append(va_adv_acc[p])
            grad_norms_adversaries[p]['va_epoch'].append(va_adv_norm_grads[p])


        ## Test set stats for this epoch
        te_vulnerabilities, te_adv_loss, te_adv_acc, te_adv_norm_grads, te_adv_images, te_adv_segmented_images, te_normal_images, te_normal_segmented_images = compute_statistics_segmenter(te_loader_untouched, net, criterion, PGD, optimizer, sample_size, True, params_dict = params_dict)

        for p in eps_stack:
            vulnerabilities[p]['te_epoch'].append(te_vulnerabilities[p])
            losses_adversaries[p]['te_epoch'].append(te_adv_loss[p])
            accuracies_adversaries[p]['te_epoch'].append(te_adv_acc[p])
            grad_norms_adversaries[p]['te_epoch'].append(te_adv_norm_grads[p])
        
        adversaries_images['train'] = tr_adv_images
        adversaries_images['val'] = va_adv_images
        adversaries_images['test'] = te_adv_images

        adversaries_segmented_images['train'] = tr_adv_segmented_images
        adversaries_segmented_images['val'] = va_adv_segmented_images
        adversaries_segmented_images['test'] = te_adv_segmented_images

        normal_images_dict['train'] = tr_normal_images
        normal_images_dict['val'] = va_normal_images
        normal_images_dict['test'] = te_normal_images

        normal_segmented_images['train'] = tr_normal_segmented_images
        normal_segmented_images['val'] = va_normal_segmented_images
        normal_segmented_images['test'] = te_normal_segmented_images


        ## Print validation stats
        print("epoch = {}, losses = {}, Train Accuracy = {}, Train grad_norms {}"
             .format(e+1, losses['tr_epoch'][-1], accuracies['tr_epoch'][-1], grad_norms['tr_epoch'][-1]))
        
        print("Epoch {}, took time = {}".format(e, time()-epoch_start_time))
        
        
        ## Saving some statistics at least val_loss
        if e==0:
            va_loss_best = copy.deepcopy(va_adv_loss[0.0])
        else:
            if va_adv_loss[0.0] <= va_loss_best:
                va_loss_best = copy.deepcopy(va_adv_loss[0.0])

                ##Saving the best model weights
                torch.save(net.state_dict(), directory_net+"best.pt")

                #Saving the best epoch in the parameters dictionary

                params_dict_save['best_epoch_loss'] = str(e)
                with open(directory+'params.txt', 'w') as file:
                    json.dump(params_dict_save, file)
                
                save_dictionaries()

### This is the PGD + segmenter combination training function

def train_pgd_and_segmenter(epochs, params_dict = params_dict, path=""):

    global tr_loader, va_loader, te_loader
    
    device = params_dict['device']
    pgd_steps = params_dict['pgd_steps']
    segmentation_training = params_dict['segmentation_training']
    PGD_training = params_dict['PGD_training']
    eps_stack = params_dict['epsilon_stack']
    attack = params_dict['attack']
    scale_capacity = params_dict['scale_capacity']
    eps = params_dict['epsilon']
    learning_rate = params_dict['learning_rate']
    dataset_size = params_dict['dataset_size']
    is_resnet = params_dict['is_resnet']
    sample_size = params_dict['sample_size']
    #sample_size = -1

    nsegs = params_dict['number_of_segments']

    print(f"Starting training with {attack}")


    if is_resnet:
        resnet = models.resnet18().to(device)
        resnet.fc = nn.Linear(512, params_dict['nClasses']).to(device)
        net = SegResnet(resnet).to(device) 
    else: 
        ## Loading fseg_nets parameters from standard net
        init_net = NetCCFC(nClasses, params_dict['scale_capacity']).to(device)
        net = fseg_NetCCFC(nClasses, params_dict['scale_capacity']).to(device)
        net.load_state_dict(init_net.state_dict(), strict=False)

    net.train()

    ## Initialize loss function and optimizer
    criterion = nn.CrossEntropyLoss() ## the loss function
    optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=5e-4) ## Optimizer
    #scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[75, 150])
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)


    ## Load corresponding attack based on given argument
    if attack is None:
        pass
    elif attack == 'FGSM':
        PGD = fb.attacks.FGSM()
    elif attack == 'L2PGD':
        PGD = fb.attacks.L2PGD(steps=pgd_steps)
    else:
        raise ValueError(f'Attack {attack} not known')

    marker_save = 0
    for e in range(epochs+1):
        ## Accumulator stats initialize to zero
        epoch_start_time = time()
        
        tr_adv_loss = {}
        tr_adv_acc = {}
        tr_adv_grads = {}
        tr_success = {}
        tr_adv_images = {}
        tr_adv_segmented_images = {}

        for i in eps_stack:
            tr_adv_loss[i] = 0
            tr_adv_acc[i] = 0
            tr_adv_grads[i] = 0
            tr_success[i] = 0
        
        total_images = 0
        start_time_10 = time()
        for i, data in enumerate(tr_loader):
            images, labels = data
            images = images.to(device)
            labels = labels.to(device)
            dtrain = time()


            ## Adding segmenter in front
            imgs2tensor_faster(net, images, nsegs, device = device, dim = 1024, nchans = 3)
            #net.fcAvg = imgs2tensor(images, nsegs, device = device, nchans = 3)
            net.train()


            ## Finding adversaries for that particular epsilon and training
            net.eval()
            fmodel = fb.PyTorchModel(net, bounds=(-1, 1))
            _, [advs], temp_success = PGD(fmodel, images, labels, epsilons=[eps])
            optimizer.zero_grad()
            if i == marker_save:
                tr_adv_images[eps] = advs.cpu().numpy() # Storing a particular batch of adversaries for that particular epsilon essentially
                tr_normal_images = images.cpu().numpy()
                tr_adv_segmented_images[eps] = net.check(advs).cpu().numpy()
                tr_normal_segmented_images = net.check(images).cpu().numpy()

            net.train()
            out_advs = net(advs)
            loss_advs = criterion(out_advs, labels)
            reps_advs = out_advs.argmax(1)
            acc_advs = ((labels == reps_advs).sum()).float()

            ## Training segmenter on that particular adversary
            loss_advs.backward()
            optimizer.step()
            optimizer.zero_grad()

            ## Losses, Accuracies, and Grad norms of adversarial images at each batch
            tr_success[eps]+=temp_success.sum().item()## total vulnerable images
            vulnerabilities[eps]['tr_batch'].append(temp_success.sum().item()/len(images))
            
            losses_adversaries[eps]['tr_batch'].append(loss_advs.item())
            accuracies_adversaries[eps]['tr_batch'].append(acc_advs.item()/len(images))
            tr_adv_loss[eps]+=loss_advs.item()*len(images)
            tr_adv_acc[eps]+=acc_advs.item()

            grad_norms_adversaries[eps]['tr_batch'].append(l2_norm_grads(advs, labels, net, criterion, optimizer))
            tr_adv_grads[eps]+=grad_norms_adversaries[eps]['tr_batch'][-1]*len(images) 


            # Getting epsilon stack that is not the epsilon we are training for 
            
            eps_stack_without_eps = list(eps_stack)
            eps_stack_without_eps.remove(eps)

            # Using this to compute statistics only for a small subset of the datapoints

            if sample_size >= total_images:
                for k in eps_stack_without_eps:
                    net.eval()
                    #fmodel = fb.PyTorchModel(net, bounds=(-1, 1))
                    _, [advs], temp_success = PGD(fmodel, images, labels, epsilons=[k])
                    optimizer.zero_grad()
                    if i == marker_save:
                        tr_adv_images[k] = advs.cpu().numpy() # Storing a particular batch of adversaries for that particular epsilon essentially
                        tr_normal_images = images.cpu().numpy()
                        tr_adv_segmented_images[k] = net.check(advs).cpu().numpy()

                    net.train()
                    out_advs = net(advs)
                    loss_advs = criterion(out_advs, labels)
                    reps_advs = out_advs.argmax(1)
                    acc_advs = ((labels == reps_advs).sum()).float()

                    ## Losses, Accuracies, and Grad norms of adversarial images at each batch
                    tr_success[k]+=temp_success.sum().item()## total vulnerable images
                    vulnerabilities[k]['tr_batch'].append(temp_success.sum().item()/len(images))
                    
                    losses_adversaries[k]['tr_batch'].append(loss_advs.item())
                    accuracies_adversaries[k]['tr_batch'].append(acc_advs.item()/len(images))
                    tr_adv_loss[k]+=loss_advs.item()*len(images)
                    tr_adv_acc[k]+=acc_advs.item()

                    grad_norms_adversaries[k]['tr_batch'].append(l2_norm_grads(advs, labels, net, criterion, optimizer))
                    tr_adv_grads[k]+=grad_norms_adversaries[k]['tr_batch'][-1]*len(images)  

            
            total_images+=len(images)
            if (i+1)%10 == 0:
                print("i = {} Accuracy_Advs = {} Loss_Advs = {} Grad_norm_natural {}".format(i+1, accuracies_adversaries[eps]['tr_batch'][-1], losses_adversaries[eps]['tr_batch'][-1], grad_norms_adversaries[eps]['tr_batch'][-1]))
                print(f"time taken for 10 batches: {time()-start_time_10}")
                start_time_10 = time()

        ## An epoch is over hence step scheduler
        if e!=0:
            scheduler.step()

        # Dividing with dataset size for stats of eps
        vulnerabilities[eps]['tr_epoch'].append(tr_success[eps]/dataset_size)
        losses_adversaries[eps]['tr_epoch'].append(tr_adv_loss[eps]/dataset_size)
        accuracies_adversaries[eps]['tr_epoch'].append(tr_adv_acc[eps]/dataset_size)
        grad_norms_adversaries[eps]['tr_epoch'].append(tr_adv_grads[eps]/dataset_size)

        # Dividing with sample size for stats of eps
        
        # Getting epsilon stack that is not the epsilon we are training for 
            
        eps_stack_without_eps = list(eps_stack)
        eps_stack_without_eps.remove(eps)
        for l in eps_stack_without_eps:
            vulnerabilities[l]['tr_epoch'].append(tr_success[l]/sample_size)
            losses_adversaries[l]['tr_epoch'].append(tr_adv_loss[l]/sample_size)
            accuracies_adversaries[l]['tr_epoch'].append(tr_adv_acc[l]/sample_size)
            grad_norms_adversaries[l]['tr_epoch'].append(tr_adv_grads[l]/sample_size)
        
        ## Validation set stats for this epoch
        va_vulnerabilities, va_adv_loss, va_adv_acc, va_adv_norm_grads, va_adv_images, va_adv_segmented_images, va_normal_images, va_normal_segmented_images = compute_statistics_pgd_and_segmenter(va_loader, net, criterion, PGD, optimizer, sample_size)

        for p in eps_stack:
            vulnerabilities[p]['va_epoch'].append(va_vulnerabilities[p])
            losses_adversaries[p]['va_epoch'].append(va_adv_loss[p])
            accuracies_adversaries[p]['va_epoch'].append(va_adv_acc[p])
            grad_norms_adversaries[p]['va_epoch'].append(va_adv_norm_grads[p])

        ## Test set stats for this epoch
        te_vulnerabilities, te_adv_loss, te_adv_acc, te_adv_norm_grads, te_adv_images, te_adv_segmented_images, te_normal_images, te_normal_segmented_images = compute_statistics_pgd_and_segmenter(te_loader, net, criterion, PGD, optimizer, sample_size)

        for p in eps_stack:
            vulnerabilities[p]['te_epoch'].append(te_vulnerabilities[p])
            losses_adversaries[p]['te_epoch'].append(te_adv_loss[p])
            accuracies_adversaries[p]['te_epoch'].append(te_adv_acc[p])
            grad_norms_adversaries[p]['te_epoch'].append(te_adv_norm_grads[p])

        ## Print validation stats
        print("epoch = {}, Train Accuracy = {}, Val Accuracy = {}, Train Vulnerability {}, Val Vulnerability {}"
             .format(e+1, accuracies_adversaries[eps]['tr_epoch'][-1], accuracies_adversaries[eps]['va_epoch'][-1], vulnerabilities[eps]['tr_epoch'][-1], vulnerabilities[eps]['va_epoch'][-1]))
        print("Epoch {}, took time = {}".format(e, time()-epoch_start_time))

        adversaries_images['train'] = tr_adv_images
        adversaries_images['val'] = va_adv_images
        adversaries_images['test'] = te_adv_images

        adversaries_segmented_images['train'] = tr_adv_segmented_images
        adversaries_segmented_images['val'] = va_adv_segmented_images
        adversaries_segmented_images['test'] = te_adv_segmented_images

        normal_images_dict['train'] = tr_normal_images
        normal_images_dict['val'] = va_normal_images
        normal_images_dict['test'] = te_normal_images

        normal_segmented_images['train'] = tr_normal_segmented_images
        normal_segmented_images['val'] = va_normal_segmented_images
        normal_segmented_images['test'] = te_normal_segmented_images

        ## Saving some statistics at least val_loss
        if e==0:
            va_loss_best = copy.deepcopy(va_adv_loss[eps])
        else:
            if va_adv_loss[eps] <= va_loss_best:
                va_loss_best = copy.deepcopy(va_adv_loss[eps])

                ##Saving the best model weights
                torch.save(net.state_dict(), directory_net+"best.pt")

                #Saving the best epoch in the parameters dictionary

                params_dict_save['best_epoch_loss'] = str(e)
                with open(directory+'params.txt', 'w') as file:
                    json.dump(params_dict_save, file)
                
                save_dictionaries()

### This function can both be used as a PGD training function, or segmentation training function

def train(epochs, params_dict = params_dict, path=""):

    global tr_loader, va_loader, te_loader
    
    device = params_dict['device']
    pgd_steps = params_dict['pgd_steps']
    segmentation_training = params_dict['segmentation_training']
    PGD_training = params_dict['PGD_training']
    eps_stack = params_dict['epsilon_stack']
    attack = params_dict['attack']
    scale_capacity = params_dict['scale_capacity']
    eps = params_dict['epsilon']
    learning_rate = params_dict['learning_rate']
    dataset_size = params_dict['dataset_size']
    is_resnet = params_dict['is_resnet']
    sample_size = params_dict['sample_size']

    print(f"Starting training with {attack}")

    if is_resnet:
        net = models.resnet18().to(device)
        net.fc = nn.Linear(512, params_dict['nClasses']).to(device)
    else:
        ## Load existing net from path if path is given
        net = NetCCFC(nClasses, params_dict['scale_capacity']).to(device)
        #net = small_net(nClasses).to(device)

    net.train()

    ## Initialize loss function and optimizer
    criterion = nn.CrossEntropyLoss() ## the loss function
    optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=5e-4) ## Optimizer

    #scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[75, 150])
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)



    ## Load corresponding attack based on given argument
    if attack is None:
        pass
    elif attack == 'FGSM':
        PGD = fb.attacks.FGSM()
    elif attack == 'L2PGD':
        PGD = fb.attacks.L2PGD(steps=pgd_steps)
    else:
        raise ValueError(f'Attack {attack} not known')

    marker_save = 0
    for e in range(epochs+1):
        ## Accumulator stats initialize to zero
        epoch_start_time = time()
        
        tr_adv_loss = {}
        tr_adv_acc = {}
        tr_adv_grads = {}
        tr_success = {}
        tr_adv_images = {}
        for i in eps_stack:
            tr_adv_loss[i] = 0
            tr_adv_acc[i] = 0
            tr_adv_grads[i] = 0
            tr_success[i] = 0
        
        total_images = 0

        start_time_10 = time()

        for i, data in enumerate(tr_loader):
            images, labels = data
            images = images.to(device)
            labels = labels.to(device)
            dtrain = time()

            net.train()

            ## Finding adversaries for that particular epsilon and training
            net.eval()
            fmodel = fb.PyTorchModel(net, bounds=(-1, 1))
            _, [advs], temp_success = PGD(fmodel, images, labels, epsilons=[eps])
            optimizer.zero_grad()
            if i == marker_save:
                tr_adv_images[eps] = advs.cpu().numpy() ## Storing a particular batch of adversaries for that particular epsilon essentially
                tr_normal_images = images.cpu().numpy()

            net.train()
            out_advs = net(advs)
            loss_advs = criterion(out_advs, labels)
            reps_advs = out_advs.argmax(1)
            acc_advs = ((labels == reps_advs).sum()).float()

            ## Training segmenter on that particular adversary
            loss_advs.backward()
            optimizer.step()
            optimizer.zero_grad()

            ## Losses, Accuracies, and Grad norms of adversarial images at each batch
            tr_success[eps]+=temp_success.sum().item() ## total vulnerable images
            vulnerabilities[eps]['tr_batch'].append(temp_success.sum().item()/len(images))
            
            losses_adversaries[eps]['tr_batch'].append(loss_advs.item())
            accuracies_adversaries[eps]['tr_batch'].append(acc_advs.item()/len(images))
            tr_adv_loss[eps]+=loss_advs.item()*len(images)
            tr_adv_acc[eps]+=acc_advs.item()

            grad_norms_adversaries[eps]['tr_batch'].append(l2_norm_grads(advs, labels, net, criterion, optimizer))
            tr_adv_grads[eps]+=grad_norms_adversaries[eps]['tr_batch'][-1]*len(images) 


            # Getting epsilon stack that is not the epsilon we are training for 
            
            eps_stack_without_eps = list(eps_stack)
            eps_stack_without_eps.remove(eps)

            # Using this to compute statistics only for a small subset of the datapoints

            if sample_size > total_images:
                for k in eps_stack_without_eps:
                    net.eval()
                    fmodel = fb.PyTorchModel(net, bounds=(-1, 1))
                    _, [advs], temp_success = PGD(fmodel, images, labels, epsilons=[k])
                    optimizer.zero_grad()
                    if i == marker_save:
                        tr_adv_images[k] = advs.cpu().numpy() # Storing a particular batch of adversaries for that particular epsilon essentially
                        tr_normal_images = images.cpu().numpy()
                    net.train()
                    out_advs = net(advs)
                    loss_advs = criterion(out_advs, labels)
                    reps_advs = out_advs.argmax(1)
                    acc_advs = ((labels == reps_advs).sum()).float()
                  

                    ## Losses, Accuracies, and Grad norms of adversarial images at each batch
                    tr_success[k]+=temp_success.sum().item()## total vulnerable images
                    vulnerabilities[k]['tr_batch'].append(temp_success.sum().item()/len(images))
                    
                    losses_adversaries[k]['tr_batch'].append(loss_advs.item())
                    accuracies_adversaries[k]['tr_batch'].append(acc_advs.item()/len(images))
                    tr_adv_loss[k]+=loss_advs.item()*len(images)
                    tr_adv_acc[k]+=acc_advs.item()

                    grad_norms_adversaries[k]['tr_batch'].append(l2_norm_grads(advs, labels, net, criterion, optimizer))
                    tr_adv_grads[k]+=grad_norms_adversaries[k]['tr_batch'][-1]*len(images)  

            
            total_images+=len(images)
            if (i+1)%50 == 0:
                print("i = {} Accuracy_Advs = {} Loss_Advs = {} Grad_norm_natural {}".format(i+1, accuracies_adversaries[eps]['tr_batch'][-1], losses_adversaries[eps]['tr_batch'][-1], grad_norms_adversaries[eps]['tr_batch'][-1]))
                print(f"time taken for 50 batches: {time()-start_time_10}")
                start_time_10 = time()
        
        ## An epoch is over hence step scheduler
        if e!=0:
            scheduler.step()

        # Dividing with dataset size for stats of eps
        
        vulnerabilities[eps]['tr_epoch'].append(tr_success[eps]/dataset_size)
        losses_adversaries[eps]['tr_epoch'].append(tr_adv_loss[eps]/dataset_size)
        accuracies_adversaries[eps]['tr_epoch'].append(tr_adv_acc[eps]/dataset_size)
        grad_norms_adversaries[eps]['tr_epoch'].append(tr_adv_grads[eps]/dataset_size)

        # Dividing with sample size for stats of eps
        
        # Getting epsilon stack that is not the epsilon we are training for 
            
        eps_stack_without_eps = list(eps_stack)
        eps_stack_without_eps.remove(eps)
        for l in eps_stack_without_eps:
            vulnerabilities[l]['tr_epoch'].append(tr_success[l]/sample_size)
            losses_adversaries[l]['tr_epoch'].append(tr_adv_loss[l]/sample_size)
            accuracies_adversaries[l]['tr_epoch'].append(tr_adv_acc[l]/sample_size)
            grad_norms_adversaries[l]['tr_epoch'].append(tr_adv_grads[l]/sample_size)
        
        ## Validation set stats for this epoch
        va_vulnerabilities, va_adv_loss, va_adv_acc, va_adv_norm_grads, va_adv_images, va_normal_images = compute_statistics(va_loader, net, criterion, PGD, fmodel, optimizer, sample_size)

        for p in eps_stack:
            vulnerabilities[p]['va_epoch'].append(va_vulnerabilities[p])
            losses_adversaries[p]['va_epoch'].append(va_adv_loss[p])
            accuracies_adversaries[p]['va_epoch'].append(va_adv_acc[p])
            grad_norms_adversaries[p]['va_epoch'].append(va_adv_norm_grads[p])

        ## Test set stats for this epoch
        te_vulnerabilities, te_adv_loss, te_adv_acc, te_adv_norm_grads, te_adv_images, te_normal_images = compute_statistics(te_loader, net, criterion, PGD, fmodel, optimizer, sample_size)

        for p in eps_stack:
            vulnerabilities[p]['te_epoch'].append(te_vulnerabilities[p])
            losses_adversaries[p]['te_epoch'].append(te_adv_loss[p])
            accuracies_adversaries[p]['te_epoch'].append(te_adv_acc[p])
            grad_norms_adversaries[p]['te_epoch'].append(te_adv_norm_grads[p])


        adversaries_images['train'] = tr_adv_images
        adversaries_images['val'] = va_adv_images
        adversaries_images['test'] = te_adv_images

        normal_images_dict['train'] = tr_normal_images
        normal_images_dict['val'] = va_normal_images
        normal_images_dict['test'] = te_normal_images

        ## Print validation stats
        print("epoch = {}, Train Accuracy = {}, Val Accuracy = {}, Train Vulnerability {}, Val Vulnerability {}"
             .format(e+1, accuracies_adversaries[eps]['tr_epoch'][-1], accuracies_adversaries[eps]['va_epoch'][-1], vulnerabilities[eps]['tr_epoch'][-1], vulnerabilities[eps]['va_epoch'][-1]))
        print("Epoch {}, took time = {}".format(e, time()-epoch_start_time))

        ## Saving some statistics at least val_loss
        if e==0:
            va_loss_best = copy.deepcopy(va_adv_loss[eps])
        else:
            if va_adv_loss[eps] <= va_loss_best:
                va_loss_best = copy.deepcopy(va_adv_loss[eps])

                ##Saving the best model weights
                torch.save(net.state_dict(), directory_net+"best.pt")

                #Saving the best epoch in the parameters dictionary

                params_dict_save['best_epoch_loss'] = str(e)
                with open(directory+'params.txt', 'w') as file:
                    json.dump(params_dict_save, file)

                save_dictionaries()

## Function that trains basically

torch.manual_seed(params_dict['seed_no'])

if params_dict['segmentation_training'] and (not params_dict['PGD_training']) and (not params_dict['PGD_and_segmenter_training']):
    print("Starting segmenter training")
    train_segmenter(params_dict['Epochs'])
elif params_dict['PGD_training'] and (not params_dict['segmentation_training'])and (not params_dict['PGD_and_segmenter_training']):
    print("Starting PGD training")
    train(params_dict['Epochs'])
elif params_dict['PGD_and_segmenter_training'] and (not params_dict['segmentation_training'])and (not params_dict['PGD_training']):
    print("Starting PGD+segmenter training")
    train_pgd_and_segmenter(params_dict['Epochs'])
else:
    pass
